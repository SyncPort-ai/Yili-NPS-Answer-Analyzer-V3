#!/usr/bin/env python3
"""
Seven Agent Analysis Workflow for NPS Report V2
Integrates the seven agent analysis into the V2 processing pipeline with AI-powered agents
"""
import json
import logging
import os
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path

from .input_data_processor import InputDataProcessor

# Configure logging
logger = logging.getLogger(__name__)

class YiliPromptTemplates:
    """Prompt templates for Yili NPS analysis following the specification format"""
    
    # Agent 1: NPS Net Value Analysis Prompt Template
    NPS_NET_VALUE_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„NPSä¸“ä¸šåˆ†æå¸ˆï¼Œæ·±åº¦åˆ†æä»¥ä¸‹NPSå‡€å€¼æ•°æ®ã€‚

NPSæ•°æ®ï¼š
å‡€å€¼åˆ†æ•°: {nps_net_score}
æ€»å“åº”æ•°: {total_responses}
è¡Œä¸šåŸºå‡†: {industry_benchmark}%

ä¼Šåˆ©äº§å“çº¿å‚è€ƒï¼š{key_brands}
ä¸»è¦ç«äº‰å¯¹æ‰‹ï¼šè’™ç‰›ã€å…‰æ˜ã€å›ä¹å®ã€ä¸‰å…ƒ

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æï¼š
1. è¯„ä¼°å½“å‰NPSå‡€å€¼çš„è¡Œä¸šåœ°ä½å’Œç«äº‰åŠ›æ°´å¹³
2. åˆ†æNPSè¡¨ç°å¯¹ä¸šåŠ¡å¥åº·åº¦çš„å½±å“
3. è¯†åˆ«NPSåˆ†æ•°èƒŒåçš„å®¢æˆ·å¿ è¯šåº¦ä¿¡å·
4. æå‡ºåŸºäºNPSæ°´å¹³çš„æˆ˜ç•¥å»ºè®®
5. é¢„æµ‹NPSè¶‹åŠ¿å’Œæ”¹è¿›ç©ºé—´

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "nps_level_assessment": {{
        "level": "ä¼˜ç§€/è‰¯å¥½/ä¸­ç­‰/éœ€æ”¹è¿›",
        "industry_comparison": "è¶…è¶Š/è¾¾åˆ°/ä½äºè¡Œä¸šæ ‡å‡†",
        "competitive_position": "é¢†å…ˆ/ä¸­ç­‰/è½å"
    }},
    "business_health_impact": {{
        "customer_loyalty": "é«˜/ä¸­/ä½",
        "brand_strength": "å¼º/ä¸­/å¼±",
        "growth_potential": "é«˜/ä¸­/ä½"
    }},
    "strategic_recommendations": [
        {{
            "priority": "é«˜/ä¸­/ä½",
            "action": "å…·ä½“è¡ŒåŠ¨å»ºè®®",
            "expected_impact": "é¢„æœŸæ•ˆæœ",
            "timeline": "å®æ–½æ—¶é—´æ¡†æ¶"
        }}
    ],
    "key_insights": [
        "æ´å¯Ÿ1: å…³é”®å‘ç°å’Œå«ä¹‰",
        "æ´å¯Ÿ2: å…³é”®å‘ç°å’Œå«ä¹‰"
    ]
}}
"""

    # Agent 2: NPS Distribution Analysis Prompt Template  
    NPS_DISTRIBUTION_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„å®¢æˆ·åˆ†ç¾¤ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹NPSåˆ†å¸ƒæ•°æ®ã€‚

å®¢æˆ·åˆ†å¸ƒæ•°æ®ï¼š
æ¨èè€…(9-10åˆ†): {promoters}äºº ({promoter_pct:.1f}%)
ä¸­ç«‹è€…(7-8åˆ†): {passives}äºº ({passive_pct:.1f}%)
è´¬æŸè€…(0-6åˆ†): {detractors}äºº ({detractor_pct:.1f}%)
æ€»å®¢æˆ·æ•°: {total}äºº

ä¼Šåˆ©ç›®æ ‡å®¢æˆ·ï¼šæ³¨é‡å¥åº·çš„æ¶ˆè´¹è€…ç¾¤ä½“ï¼Œè¿½æ±‚å“è´¨ç”Ÿæ´»çš„ä¸­äº§é˜¶çº§

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æï¼š
1. è¯„ä¼°å®¢æˆ·åˆ†å¸ƒç»“æ„çš„å¥åº·åº¦
2. è¯†åˆ«å„å®¢æˆ·ç¾¤ä½“çš„ç‰¹å¾å’Œè¡Œä¸ºæ¨¡å¼
3. åˆ†æå®¢æˆ·æµè½¬çš„å¯èƒ½æ€§å’Œé£é™©
4. æå‡ºå®¢æˆ·ç¾¤ä½“ä¼˜åŒ–ç­–ç•¥
5. é¢„æµ‹å®¢æˆ·ä»·å€¼å’Œç”Ÿå‘½å‘¨æœŸ

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "distribution_health": {{
        "structure_assessment": "å¥åº·/ä¸­ç­‰/æœ‰é£é™©",
        "dominant_group": "æ¨èè€…/ä¸­ç«‹è€…/è´¬æŸè€…",
        "balance_score": "å¹³è¡¡è¯„åˆ†0-10"
    }},
    "customer_insights": {{
        "promoter_characteristics": ["æ¨èè€…ç‰¹å¾1", "æ¨èè€…ç‰¹å¾2"],
        "passive_conversion_potential": "é«˜/ä¸­/ä½",
        "detractor_risk_level": "é«˜/ä¸­/ä½"
    }},
    "optimization_strategies": [
        {{
            "target_group": "æ¨èè€…/ä¸­ç«‹è€…/è´¬æŸè€…",
            "strategy": "å…·ä½“ä¼˜åŒ–ç­–ç•¥",
            "conversion_goal": "è½¬åŒ–ç›®æ ‡",
            "success_metrics": "æˆåŠŸè¡¡é‡æŒ‡æ ‡"
        }}
    ],
    "business_implications": [
        "å«ä¹‰1: å¯¹ä¸šåŠ¡çš„å½±å“",
        "å«ä¹‰2: å¯¹ä¸šåŠ¡çš„å½±å“"
    ]
}}
"""

    # Agent 3: Positive Multiple Choice Analysis Prompt Template
    POSITIVE_FACTORS_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„å®¢æˆ·æ´å¯Ÿä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹æ¨èè€…çš„æ­£é¢å› ç´ é€‰æ‹©ã€‚

æ­£é¢å› ç´ æ•°æ®ï¼š
æ€»é€‰æ‹©æ•°: {total_selections}
å› ç´ ç§ç±»: {unique_factors}
å› ç´ é¢‘æ¬¡: {factor_frequency}
å®Œæ•´å› ç´ åˆ—è¡¨: {flattened_text}

ä¼Šåˆ©äº§å“ä¼˜åŠ¿é¢†åŸŸï¼šäº§å“å“è´¨ã€å£å‘³åˆ›æ–°ã€è¥å…»å¥åº·ã€åŒ…è£…è®¾è®¡ã€å“ç‰Œä¿¡ä»»åº¦

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æï¼š
1. è¯†åˆ«å®¢æˆ·æœ€è®¤å¯çš„æ ¸å¿ƒä¼˜åŠ¿å› ç´ 
2. åˆ†æå› ç´ èƒŒåçš„å®¢æˆ·ä»·å€¼éœ€æ±‚
3. è¯„ä¼°è¿™äº›ä¼˜åŠ¿çš„å¸‚åœºå·®å¼‚åŒ–ä»·å€¼
4. æå‡ºåŸºäºä¼˜åŠ¿çš„è¥é”€ç­–ç•¥å»ºè®®
5. è¯†åˆ«å¯æ”¾å¤§çš„ç«äº‰ä¼˜åŠ¿

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "core_advantages": {{
        "primary_strength": "æœ€æ ¸å¿ƒä¼˜åŠ¿",
        "secondary_strengths": ["æ¬¡è¦ä¼˜åŠ¿1", "æ¬¡è¦ä¼˜åŠ¿2"],
        "advantage_categories": {{"ç±»åˆ«1": æƒé‡, "ç±»åˆ«2": æƒé‡}}
    }},
    "customer_value_drivers": {{
        "functional_benefits": ["åŠŸèƒ½æ€§ä»·å€¼1", "åŠŸèƒ½æ€§ä»·å€¼2"],
        "emotional_benefits": ["æƒ…æ„Ÿæ€§ä»·å€¼1", "æƒ…æ„Ÿæ€§ä»·å€¼2"],
        "social_benefits": ["ç¤¾äº¤æ€§ä»·å€¼1", "ç¤¾äº¤æ€§ä»·å€¼2"]
    }},
    "marketing_opportunities": [
        {{
            "advantage": "ä¼˜åŠ¿å› ç´ ",
            "communication_message": "ä¼ æ’­ä¿¡æ¯",
            "target_audience": "ç›®æ ‡å—ä¼—",
            "channels": ["ä¼ æ’­æ¸ é“1", "ä¼ æ’­æ¸ é“2"]
        }}
    ],
    "competitive_positioning": {{
        "unique_selling_points": ["ç‹¬ç‰¹å–ç‚¹1", "ç‹¬ç‰¹å–ç‚¹2"],
        "differentiation_strategy": "å·®å¼‚åŒ–ç­–ç•¥å»ºè®®"
    }}
}}
"""

    # Agent 4: Positive Free Answers Analysis Prompt Template
    POSITIVE_OPENTEXT_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„å®¢æˆ·ä½“éªŒä¸“å®¶ï¼Œæ·±åº¦åˆ†æä»¥ä¸‹æ¨èè€…çš„å¼€æ”¾å›ç­”ã€‚

å¼€æ”¾å›ç­”æ•°æ®ï¼š
å›ç­”æ•°é‡: {total_responses}
å›ç­”ç‡: {response_rate:.1f}%
å®Œæ•´å›ç­”å†…å®¹: {flattened_text}

åˆ†æé‡ç‚¹ï¼šå®¢æˆ·çš„çœŸå®æ„Ÿå—ã€æƒ…æ„Ÿè¿æ¥ã€ä½¿ç”¨åœºæ™¯ã€æ¨èåŠ¨æœº

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æï¼š
1. æå–å®¢æˆ·çš„çœŸå®æƒ…æ„Ÿå’Œæ€åº¦
2. è¯†åˆ«äº§å“ä½¿ç”¨çš„å…·ä½“åœºæ™¯å’Œæ—¶æœº
3. åˆ†æå®¢æˆ·ä¸å“ç‰Œçš„æƒ…æ„Ÿè¿æ¥ç‚¹
4. å‘ç°éšè—çš„å®¢æˆ·éœ€æ±‚å’ŒæœŸæœ›
5. è¯†åˆ«å¯ç”¨äºè¥é”€çš„å®¢æˆ·æ•…äº‹

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "emotional_insights": {{
        "dominant_emotions": ["ä¸»å¯¼æƒ…æ„Ÿ1", "ä¸»å¯¼æƒ…æ„Ÿ2"],
        "satisfaction_drivers": ["æ»¡æ„é©±åŠ¨å› ç´ 1", "æ»¡æ„é©±åŠ¨å› ç´ 2"],
        "emotional_connection_strength": "å¼º/ä¸­/å¼±"
    }},
    "usage_scenarios": {{
        "primary_occasions": ["ä½¿ç”¨åœºæ™¯1", "ä½¿ç”¨åœºæ™¯2"],
        "consumption_patterns": ["æ¶ˆè´¹æ¨¡å¼1", "æ¶ˆè´¹æ¨¡å¼2"],
        "decision_contexts": ["å†³ç­–åœºæ™¯1", "å†³ç­–åœºæ™¯2"]
    }},
    "hidden_needs": {{
        "unmet_expectations": ["æœªæ»¡è¶³æœŸæœ›1", "æœªæ»¡è¶³æœŸæœ›2"],
        "improvement_opportunities": ["æ”¹è¿›æœºä¼š1", "æ”¹è¿›æœºä¼š2"],
        "innovation_directions": ["åˆ›æ–°æ–¹å‘1", "åˆ›æ–°æ–¹å‘2"]
    }},
    "customer_stories": [
        {{
            "story_theme": "æ•…äº‹ä¸»é¢˜",
            "narrative": "å®¢æˆ·å™è¿°",
            "marketing_value": "è¥é”€ä»·å€¼",
            "usage_recommendation": "ä½¿ç”¨å»ºè®®"
        }}
    ]
}}
"""

    # Agent 5: Negative Multiple Choice Analysis Prompt Template
    NEGATIVE_FACTORS_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„é—®é¢˜è¯Šæ–­ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹éæ¨èè€…çš„è´Ÿé¢å› ç´ é€‰æ‹©ã€‚

è´Ÿé¢å› ç´ æ•°æ®ï¼š
æ€»é€‰æ‹©æ•°: {total_selections}
é—®é¢˜ç§ç±»: {unique_factors}
å› ç´ é¢‘æ¬¡: {factor_frequency}
å®Œæ•´å› ç´ åˆ—è¡¨: {flattened_text}

å¸¸è§é—®é¢˜é¢†åŸŸï¼šäº§å“è´¨é‡ã€ä»·æ ¼ç­–ç•¥ã€è¥é”€å®£ä¼ ã€æ¸ é“ä¾¿åˆ©æ€§ã€æœåŠ¡ä½“éªŒ

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æï¼š
1. è¯†åˆ«æœ€ä¸¥é‡çš„å®¢æˆ·æµå¤±é£é™©å› ç´ 
2. åˆ†æé—®é¢˜çš„æ ¹æœ¬åŸå› å’Œç³»ç»Ÿæ€§ç¼ºé™·
3. è¯„ä¼°é—®é¢˜è§£å†³çš„ç´§æ€¥ç¨‹åº¦å’Œå½±å“èŒƒå›´
4. æå‡ºé—®é¢˜è§£å†³çš„ä¼˜å…ˆçº§å’Œèµ„æºé…ç½®å»ºè®®
5. é¢„æµ‹ä¸è§£å†³é—®é¢˜çš„ä¸šåŠ¡é£é™©

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "critical_issues": {{
        "primary_problem": "æœ€ä¸¥é‡é—®é¢˜",
        "secondary_problems": ["æ¬¡è¦é—®é¢˜1", "æ¬¡è¦é—®é¢˜2"],
        "issue_categories": {{"ç±»åˆ«1": ä¸¥é‡ç¨‹åº¦, "ç±»åˆ«2": ä¸¥é‡ç¨‹åº¦}}
    }},
    "root_cause_analysis": {{
        "system_failures": ["ç³»ç»Ÿæ€§é—®é¢˜1", "ç³»ç»Ÿæ€§é—®é¢˜2"],
        "process_gaps": ["æµç¨‹ç¼ºé™·1", "æµç¨‹ç¼ºé™·2"],
        "resource_constraints": ["èµ„æºé™åˆ¶1", "èµ„æºé™åˆ¶2"]
    }},
    "solution_roadmap": [
        {{
            "problem": "å…·ä½“é—®é¢˜",
            "solution": "è§£å†³æ–¹æ¡ˆ",
            "priority": "é«˜/ä¸­/ä½",
            "timeline": "è§£å†³æ—¶é—´æ¡†æ¶",
            "resources_needed": "æ‰€éœ€èµ„æº",
            "success_metrics": "æˆåŠŸæŒ‡æ ‡"
        }}
    ],
    "risk_assessment": {{
        "customer_churn_risk": "é«˜/ä¸­/ä½",
        "brand_reputation_impact": "é«˜/ä¸­/ä½",
        "revenue_impact": "é«˜/ä¸­/ä½",
        "competitive_vulnerability": "é«˜/ä¸­/ä½"
    }}
}}
"""

    # Agent 6: Negative Free Answers Analysis Prompt Template
    NEGATIVE_OPENTEXT_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„å®¢æˆ·æŒ½ç•™ä¸“å®¶ï¼Œæ·±åº¦åˆ†æä»¥ä¸‹éæ¨èè€…çš„å¼€æ”¾å›ç­”ã€‚

å¼€æ”¾å›ç­”æ•°æ®ï¼š
å›ç­”æ•°é‡: {total_responses}
å›ç­”ç‡: {response_rate:.1f}%
å®Œæ•´å›ç­”å†…å®¹: {flattened_text}

åˆ†æé‡ç‚¹ï¼šå®¢æˆ·çš„ä¸æ»¡æƒ…ç»ªã€å…·ä½“ç—›ç‚¹ã€å¤±æœ›åŸå› ã€æœŸæœ›è½å·®

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æï¼š
1. è¯†åˆ«å®¢æˆ·ä¸æ»¡çš„æ·±å±‚æ¬¡åŸå› 
2. åˆ†æå®¢æˆ·æœŸæœ›ä¸å®é™…ä½“éªŒçš„å·®è·
3. è¯„ä¼°å®¢æˆ·æƒ…æ„Ÿåˆ›ä¼¤çš„ä¸¥é‡ç¨‹åº¦
4. æå‡ºå®¢æˆ·æŒ½ç•™å’Œå…³ç³»ä¿®å¤ç­–ç•¥
5. å‘ç°äº§å“å’ŒæœåŠ¡æ”¹è¿›çš„å…·ä½“æ–¹å‘

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "dissatisfaction_analysis": {{
        "primary_complaints": ["ä¸»è¦æŠ±æ€¨1", "ä¸»è¦æŠ±æ€¨2"],
        "emotional_intensity": "å¼ºçƒˆ/ä¸­ç­‰/è½»å¾®",
        "disappointment_sources": ["å¤±æœ›æ¥æº1", "å¤±æœ›æ¥æº2"]
    }},
    "expectation_gaps": {{
        "quality_expectations": ["è´¨é‡æœŸæœ›1", "è´¨é‡æœŸæœ›2"],
        "service_expectations": ["æœåŠ¡æœŸæœ›1", "æœåŠ¡æœŸæœ›2"],
        "value_expectations": ["ä»·å€¼æœŸæœ›1", "ä»·å€¼æœŸæœ›2"]
    }},
    "retention_strategies": [
        {{
            "customer_segment": "å®¢æˆ·ç¾¤ä½“",
            "retention_approach": "æŒ½ç•™æ–¹æ³•",
            "compensation_needed": "è¡¥å¿æªæ–½",
            "relationship_repair": "å…³ç³»ä¿®å¤ç­–ç•¥",
            "success_probability": "æˆåŠŸæ¦‚ç‡"
        }}
    ],
    "improvement_priorities": [
        {{
            "area": "æ”¹è¿›é¢†åŸŸ",
            "specific_issue": "å…·ä½“é—®é¢˜",
            "improvement_action": "æ”¹è¿›è¡ŒåŠ¨",
            "impact_level": "å½±å“ç¨‹åº¦",
            "implementation_difficulty": "å®æ–½éš¾åº¦"
        }}
    ]
}}
"""

    # Agent 7: Comprehensive Summary Analysis Prompt Template
    COMPREHENSIVE_SUMMARY_ANALYSIS = """
è¯·ä½œä¸ºä¼Šåˆ©é›†å›¢çš„é«˜çº§æˆ˜ç•¥åˆ†æå¸ˆï¼ŒåŸºäºå‰å…­ä¸ªæ™ºèƒ½ä½“çš„åˆ†æç»“æœï¼Œæä¾›ç»¼åˆæ€§çš„NPSæ´å¯Ÿå’Œæˆ˜ç•¥å»ºè®®ã€‚

å‰å…­ä¸ªæ™ºèƒ½ä½“åˆ†ææ‘˜è¦ï¼š
NPSå‡€å€¼åˆ†æ: {agent1_summary}
å®¢æˆ·åˆ†å¸ƒåˆ†æ: {agent2_summary}  
æ­£é¢å› ç´ åˆ†æ: {agent3_summary}
æ­£é¢å¼€æ”¾å›ç­”: {agent4_summary}
è´Ÿé¢å› ç´ åˆ†æ: {agent5_summary}
è´Ÿé¢å¼€æ”¾å›ç­”: {agent6_summary}

ä¼Šåˆ©é›†å›¢èƒŒæ™¯ï¼šä¸­å›½ä¹³åˆ¶å“å¸‚åœºé¢†å¯¼è€…ï¼Œä¸»è¦å“ç‰ŒåŒ…æ‹¬{key_brands}ï¼Œé¢ä¸´è’™ç‰›ã€å…‰æ˜ç­‰æ¿€çƒˆç«äº‰

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚æä¾›ç»¼åˆåˆ†æï¼š
1. æ•´åˆæ‰€æœ‰åˆ†æç»´åº¦ï¼Œè¯†åˆ«NPSè¡¨ç°çš„æ ¸å¿ƒé©±åŠ¨å› ç´ 
2. åˆ¶å®šå…¨é¢çš„NPSæå‡æˆ˜ç•¥å’Œè¡ŒåŠ¨è®¡åˆ’
3. å¹³è¡¡çŸ­æœŸæ”¹è¿›å’Œé•¿æœŸå“ç‰Œå»ºè®¾éœ€æ±‚
4. æä¾›è·¨éƒ¨é—¨åä½œçš„å…·ä½“å»ºè®®
5. å»ºç«‹NPSç›‘æ§å’ŒæŒç»­æ”¹è¿›æœºåˆ¶

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "comprehensive_assessment": {{
        "overall_nps_health": "å¥åº·/ä¸­ç­‰/æœ‰é£é™©",
        "key_drivers": ["æ ¸å¿ƒé©±åŠ¨å› ç´ 1", "æ ¸å¿ƒé©±åŠ¨å› ç´ 2"],
        "critical_success_factors": ["æˆåŠŸå…³é”®å› ç´ 1", "æˆåŠŸå…³é”®å› ç´ 2"],
        "main_challenges": ["ä¸»è¦æŒ‘æˆ˜1", "ä¸»è¦æŒ‘æˆ˜2"]
    }},
    "strategic_roadmap": {{
        "immediate_actions": [
            {{
                "action": "ç«‹å³è¡ŒåŠ¨",
                "timeline": "1-3ä¸ªæœˆ",
                "owner": "è´Ÿè´£éƒ¨é—¨",
                "success_metric": "æˆåŠŸæŒ‡æ ‡"
            }}
        ],
        "medium_term_initiatives": [
            {{
                "initiative": "ä¸­æœŸä¸¾æª",
                "timeline": "3-12ä¸ªæœˆ", 
                "investment_required": "æ‰€éœ€æŠ•èµ„",
                "expected_outcome": "é¢„æœŸç»“æœ"
            }}
        ],
        "long_term_vision": {{
            "target_nps": "ç›®æ ‡NPSåˆ†æ•°",
            "timeline": "12-24ä¸ªæœˆ",
            "transformation_areas": ["è½¬å‹é¢†åŸŸ1", "è½¬å‹é¢†åŸŸ2"]
        }}
    }},
    "cross_functional_recommendations": {{
        "product_development": ["äº§å“å¼€å‘å»ºè®®1", "äº§å“å¼€å‘å»ºè®®2"],
        "marketing_communications": ["è¥é”€ä¼ æ’­å»ºè®®1", "è¥é”€ä¼ æ’­å»ºè®®2"],
        "customer_service": ["å®¢æˆ·æœåŠ¡å»ºè®®1", "å®¢æˆ·æœåŠ¡å»ºè®®2"],
        "sales_channels": ["é”€å”®æ¸ é“å»ºè®®1", "é”€å”®æ¸ é“å»ºè®®2"],
        "quality_management": ["è´¨é‡ç®¡ç†å»ºè®®1", "è´¨é‡ç®¡ç†å»ºè®®2"]
    }},
    "monitoring_framework": {{
        "kpi_dashboard": ["å…³é”®æŒ‡æ ‡1", "å…³é”®æŒ‡æ ‡2"],
        "feedback_mechanisms": ["åé¦ˆæœºåˆ¶1", "åé¦ˆæœºåˆ¶2"],
        "review_frequency": "è¯„ä¼°é¢‘ç‡",
        "escalation_triggers": ["å‡çº§è§¦å‘æ¡ä»¶1", "å‡çº§è§¦å‘æ¡ä»¶2"]
    }},
    "executive_summary": {{
        "situation": "å½“å‰æƒ…å†µæ¦‚è¿°",
        "key_insights": "å…³é”®æ´å¯Ÿ",
        "recommendations": "æ ¸å¿ƒå»ºè®®",
        "expected_impact": "é¢„æœŸå½±å“"
    }}
}}
"""

class YiliAIClient:
    """AI client for Yili NPS analysis with dual gateway support"""
    
    def __init__(self, use_yili_gateway: bool = True):
        self.use_yili_gateway = use_yili_gateway
        self.max_retries = 3
        
        # Yili Gateway Configuration
        self.yili_gateway_url = os.getenv(
            "YILI_GATEWAY_URL", 
            "https://ycsb-gw-pub.xapi.digitalyili.com/restcloud/yili-gpt-prod/v1/getTextToThird"
        )
        self.yili_app_key = os.getenv("YILI_APP_KEY", "649aa4671fa7b91962caa01d")
        
        # Azure OpenAI Configuration (fallback)
        self.azure_endpoint = os.getenv(
            "AZURE_OPENAI_ENDPOINT",
            "https://gpt4-turbo-sweden.openai.azure.com/openai/deployments/only_for_yili_test_4o_240710/chat/completions"
        )
        self.azure_api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
        
        # Import requests here to avoid issues if not available
        try:
            import requests
            self.session = requests.Session()
        except ImportError:
            logger.warning("requestsåº“æœªå®‰è£…ï¼ŒAIåŠŸèƒ½å°†è¢«ç¦ç”¨")
            self.session = None
    
    def analyze_with_prompt(self, prompt_template: str, **kwargs) -> Dict[str, Any]:
        """
        Analyze data using AI with the given prompt template
        
        Args:
            prompt_template: The prompt template to use
            **kwargs: Variables to substitute in the template
            
        Returns:
            Dict: AI analysis results
        """
        if not self.session:
            logger.warning("AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
            return self._generate_fallback_response(prompt_template, **kwargs)
        
        try:
            # Format the prompt with provided variables
            formatted_prompt = prompt_template.format(**kwargs)
            
            # Try Yili Gateway first if enabled
            if self.use_yili_gateway:
                try:
                    result = self._call_yili_gateway(formatted_prompt)
                    if result:
                        return self._parse_ai_response(result)
                except Exception as e:
                    logger.warning(f"ä¼Šåˆ©ç½‘å…³è°ƒç”¨å¤±è´¥: {e}")
            
            # Fallback to Azure OpenAI
            try:
                result = self._call_azure_openai(formatted_prompt)
                if result:
                    return self._parse_ai_response(result)
            except Exception as e:
                logger.warning(f"Azure OpenAIè°ƒç”¨å¤±è´¥: {e}")
            
            # Final fallback to rule-based analysis
            return self._generate_fallback_response(prompt_template, **kwargs)
            
        except Exception as e:
            logger.error(f"AIåˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            return self._generate_fallback_response(prompt_template, **kwargs)
    
    def _call_yili_gateway(self, prompt: str) -> str:
        """Call Yili AI Gateway"""
        data = {
            "channelCode": "wvEO",
            "tenantsCode": "Yun8457",
            "choiceModel": 1,
            "isMultiSession": 1,
            "requestContent": prompt,
            "requestType": 1,
            "streamFlag": 0,
            "userCode": "wvEO10047252",
            "requestGroupCode": "1243112808144896"
        }
        
        headers = {
            "Content-Type": "application/json",
            "appKey": self.yili_app_key
        }
        
        response = self.session.post(
            self.yili_gateway_url, 
            json=data, 
            headers=headers, 
            timeout=30
        )
        
        if response.status_code == 200:
            response_data = response.json()
            if response_data.get('code') == 0:
                return response_data['data']['responseVO']
        
        raise Exception(f"ä¼Šåˆ©ç½‘å…³è¿”å›é”™è¯¯: {response.status_code}")
    
    def _call_azure_openai(self, prompt: str) -> str:
        """Call Azure OpenAI as fallback"""
        headers = {
            "Content-Type": "application/json",
            "api-key": self.azure_api_key
        }
        
        data = {
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1,
            "max_tokens": 4000
        }
        
        response = self.session.post(
            self.azure_endpoint,
            json=data,
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 200:
            response_data = response.json()
            return response_data['choices'][0]['message']['content']
        
        raise Exception(f"Azure OpenAIè¿”å›é”™è¯¯: {response.status_code}")
    
    def _parse_ai_response(self, response_text: str) -> Dict[str, Any]:
        """Parse AI response and extract JSON"""
        try:
            # Try to find JSON in the response
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            else:
                # If no JSON found, return the text as insights
                return {
                    "analysis_text": response_text,
                    "key_insights": [response_text[:200] + "..." if len(response_text) > 200 else response_text]
                }
        except Exception as e:
            logger.warning(f"AIå“åº”è§£æå¤±è´¥: {e}")
            return {
                "analysis_text": response_text,
                "parsing_error": str(e),
                "key_insights": ["AIåˆ†æç»“æœè§£æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸå§‹æ–‡æœ¬"]
            }
    
    def _generate_fallback_response(self, prompt_template: str, **kwargs) -> Dict[str, Any]:
        """Generate fallback response when AI is not available"""
        return {
            "fallback_mode": True,
            "message": "AIæœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™åŒ–åˆ†æ",
            "key_insights": [
                "æ•°æ®åˆ†æåŸºäºè§„åˆ™åŒ–é€»è¾‘å®Œæˆ",
                "å»ºè®®è”ç³»æŠ€æœ¯æ”¯æŒå¯ç”¨AIåŠŸèƒ½ä»¥è·å¾—æ›´æ·±åº¦çš„æ´å¯Ÿ"
            ],
            "analysis_text": "AIåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œå·²ä½¿ç”¨å¤‡ç”¨åˆ†ææ–¹æ³•"
        }


class SevenAgentWorkflow:
    """Seven Agent Analysis Workflow for NPS Report V2 with AI-powered agents"""
    
    def __init__(self, use_ai: bool = True):
        self.use_ai = use_ai
        self.ai_client = YiliAIClient() if use_ai else None
        
        self.question_texts = {
            "nps_question": "Q1æ‚¨å‘æœ‹å‹æˆ–åŒäº‹æ¨èæˆ‘ä»¬çš„å¯èƒ½æ€§å¤šå¤§ï¼Ÿ",
            "negative_factors": "Q2 æ‚¨ä¸æ„¿æ„æ¨èæˆ‘ä»¬çš„ä¸»è¦å› ç´ æœ‰å“ªäº›ï¼Ÿ",
            "negative_specific": "Q3 æ‚¨ä¸æ„¿æ„æ¨èæˆ‘ä»¬çš„å…·ä½“åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
            "positive_factors": "Q4 æ‚¨æ„¿æ„æ¨èæˆ‘ä»¬çš„ä¸»è¦å› ç´ æœ‰å“ªäº›ï¼Ÿ",
            "positive_specific": "Q5 æ‚¨æ„¿æ„æ¨èæˆ‘ä»¬çš„å…·ä½“åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
        }
        
        # Load knowledge files for context-aware analysis
        self.knowledge_base = self._load_knowledge_files()
        
        # Enhanced additional information with knowledge base
        self.additional_info = {
            "product_context": "ä¼Šåˆ©é›†å›¢ä¹³åˆ¶å“å…¨äº§å“çº¿",
            "market_context": "ä¸­å›½ä¹³åˆ¶å“å¸‚åœºç«äº‰æ¿€çƒˆï¼Œä¸»è¦ç«äº‰å¯¹æ‰‹åŒ…æ‹¬è’™ç‰›ã€å…‰æ˜ã€å›ä¹å®",
            "business_focus": "æå‡NPSåˆ†æ•°ï¼Œæ”¹å–„å®¢æˆ·æ»¡æ„åº¦ï¼Œä¼˜åŒ–äº§å“å’ŒæœåŠ¡ä½“éªŒ",
            "target_segments": "æ³¨é‡å¥åº·çš„æ¶ˆè´¹è€…ç¾¤ä½“ï¼Œè¿½æ±‚å“è´¨ç”Ÿæ´»çš„ä¸­äº§é˜¶çº§",
            "key_brands": self.knowledge_base.get("key_brands", []),
            "business_units": [unit["name"] for unit in self.knowledge_base.get("business_units", [])],
            "analysis_tags": self.knowledge_base.get("business_tags", {})
        }
    
    def _load_knowledge_files(self):
        """Load all knowledge files for context-aware analysis"""
        knowledge_base = {}
        knowledge_dir = Path(__file__).resolve().parent.parent / "data" / "knowledge_files"
        
        try:
            # Load business tags
            tags_file = knowledge_dir / "business_tags.json"
            if tags_file.exists():
                with open(tags_file, 'r', encoding='utf-8') as f:
                    knowledge_base["business_tags"] = json.load(f)
            
            # Load Yili products
            products_file = knowledge_dir / "yili_products.json"
            if products_file.exists():
                with open(products_file, 'r', encoding='utf-8') as f:
                    products_data = json.load(f)
                    knowledge_base["key_brands"] = products_data.get("key_brands", [])
                    knowledge_base["business_units"] = products_data.get("business_units", [])
            
            # Load business rules
            rules_file = knowledge_dir / "business_rules.json"
            if rules_file.exists():
                with open(rules_file, 'r', encoding='utf-8') as f:
                    knowledge_base["business_rules"] = json.load(f)
            
            logger.info(f"çŸ¥è¯†åº“åŠ è½½æˆåŠŸ: {len(knowledge_base)} ä¸ªçŸ¥è¯†æ–‡ä»¶")
            
        except Exception as e:
            logger.warning(f"çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
            # Fallback to basic knowledge
            knowledge_base = {
                "business_tags": {
                    "å£å‘³è®¾è®¡": ["å£å‘³", "é…¸å‘³", "ç”œå‘³", "å¥¶é¦™å‘³", "å£æ„Ÿ", "ç»†è…»åº¦"],
                    "åŒ…è£…è®¾è®¡": ["åŒ…è£…", "ç¾è§‚", "ä¾¿åˆ©æ€§", "è‰²å½©", "å›¾æ¡ˆ"],
                    "æ¦‚å¿µè®¾è®¡": ["å“è´¨", "å¥åº·", "è¥å…»", "é«˜ç«¯", "ä»·æ ¼"]
                },
                "key_brands": ["å®‰æ…•å¸Œ", "é‡‘å…¸", "èˆ’åŒ–", "QQæ˜Ÿ", "ä¼˜é…¸ä¹³", "å·§ä¹å…¹"]
            }
        
        return knowledge_base
    
    def _categorize_factors(self, factors: List[str], factor_type: str = "positive"):
        """Categorize factors using business tags knowledge"""
        business_tags = self.knowledge_base.get("business_tags", {})
        categories = {}
        
        for factor in factors:
            for category, keywords in business_tags.items():
                for keyword in keywords:
                    if keyword in factor:
                        categories[category] = categories.get(category, 0) + 1
                        break
        
        # If no matches found, use basic categorization
        if not categories:
            for factor in factors:
                if any(word in factor for word in ["å£å‘³", "å£æ„Ÿ", "å¥½å–", "å‘³é“", "ç”œå‘³", "é…¸å‘³"]):
                    categories["å£å‘³è®¾è®¡"] = categories.get("å£å‘³è®¾è®¡", 0) + 1
                elif any(word in factor for word in ["åŒ…è£…", "ç¾è§‚", "è®¾è®¡", "å¤–è§‚", "ä¾¿æº", "æ–¹ä¾¿"]):
                    categories["åŒ…è£…è®¾è®¡"] = categories.get("åŒ…è£…è®¾è®¡", 0) + 1
                elif any(word in factor for word in ["ä»·æ ¼", "æ€§ä»·æ¯”", "ä¾¿å®œ", "å®æƒ ", "å€¼"]):
                    categories["æ¦‚å¿µè®¾è®¡"] = categories.get("æ¦‚å¿µè®¾è®¡", 0) + 1
                else:
                    categories["å…¶ä»–"] = categories.get("å…¶ä»–", 0) + 1
        
        return categories
    
    def process_nps_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main processing function for seven agent analysis
        
        Args:
            input_data: Raw input data (Chinese format or other)
            
        Returns:
            Dict: Complete seven agent analysis results
        """
        try:
            logger.info("å¼€å§‹ä¸ƒæ™ºèƒ½ä½“NPSåˆ†ææµç¨‹...")
            
            # Step 1: Process input data
            processor = InputDataProcessor()
            processed_data = processor.process_survey_data(input_data)
            
            # Step 2: Split data into positive/negative sets
            positive_set, negative_set = self._split_users_2x3(processed_data)
            
            # Step 3: Calculate summary statistics
            data_summary = self._calculate_summary_stats(positive_set, negative_set)
            
            # Step 4: Run seven agent analysis
            agent_results = self._run_seven_agents(positive_set, negative_set, data_summary)
            
            # Step 5: Create final output
            final_output = self._create_final_output(
                input_data, processed_data, agent_results, positive_set, negative_set
            )
            
            logger.info("ä¸ƒæ™ºèƒ½ä½“NPSåˆ†æå®Œæˆ")
            return final_output
            
        except Exception as e:
            logger.error(f"ä¸ƒæ™ºèƒ½ä½“åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            raise
    
    def _split_users_2x3(self, processed_data):
        """Split users into 2 sets x 3 arrays each"""
        
        positive_set = {
            "nps_array": [],
            "multiple_choice_array": [],
            "free_question_array": []
        }
        
        negative_set = {
            "nps_array": [],
            "multiple_choice_array": [],
            "free_question_array": []
        }
        
        for response in processed_data.response_data:
            user_id = response.user_id
            nps_score = response.nps_score.value
            
            # NPS data
            nps_data = {
                "user_id": user_id,
                "nps_score": nps_score,
                "nps_category": response.nps_score.category.value,
                "raw_value": response.nps_score.raw_value
            }
            
            # Multiple choice data
            multiple_choice_data = {
                "user_id": user_id,
                "negative_factors": response.factor_responses.get('negative_factors').selected if response.factor_responses.get('negative_factors') else [],
                "positive_factors": response.factor_responses.get('positive_factors').selected if response.factor_responses.get('positive_factors') else [],
                "negative_factor_count": len(response.factor_responses.get('negative_factors').selected) if response.factor_responses.get('negative_factors') else 0,
                "positive_factor_count": len(response.factor_responses.get('positive_factors').selected) if response.factor_responses.get('positive_factors') else 0
            }
            
            # Free question data
            specific_reasons = response.open_responses.specific_reasons if response.open_responses.specific_reasons else {}
            free_question_data = {
                "user_id": user_id,
                "negative_reason": specific_reasons.get('negative', ''),
                "positive_reason": specific_reasons.get('positive', ''),
                "has_negative_text": bool(specific_reasons.get('negative', '').strip()),
                "has_positive_text": bool(specific_reasons.get('positive', '').strip()),
                "total_text_length": len(specific_reasons.get('negative', '') + specific_reasons.get('positive', ''))
            }
            
            # Categorize into positive or negative set
            if nps_score >= 9:  # Promoters
                positive_set["nps_array"].append(nps_data)
                positive_set["multiple_choice_array"].append(multiple_choice_data)
                positive_set["free_question_array"].append(free_question_data)
            else:  # Detractors and Passives
                negative_set["nps_array"].append(nps_data)
                negative_set["multiple_choice_array"].append(multiple_choice_data)
                negative_set["free_question_array"].append(free_question_data)
        
        return positive_set, negative_set
    
    def _calculate_summary_stats(self, positive_set, negative_set):
        """Calculate summary statistics"""
        
        total_users = len(positive_set["nps_array"]) + len(negative_set["nps_array"])
        promoters = len(positive_set["nps_array"])
        others = len(negative_set["nps_array"])
        
        promoter_pct = (promoters / total_users * 100) if total_users > 0 else 0
        
        # Calculate detractors and passives within negative set
        detractors = len([x for x in negative_set["nps_array"] if x["nps_score"] <= 6])
        passives = len([x for x in negative_set["nps_array"] if x["nps_score"] >= 7])
        
        detractor_pct = (detractors / total_users * 100) if total_users > 0 else 0
        nps_net_score = promoter_pct - detractor_pct
        
        return {
            "total_users": total_users,
            "positive_users": promoters,
            "negative_users": others,
            "nps_net_score": nps_net_score,
            "promoters": promoters,
            "passives": passives,
            "detractors": detractors
        }
    
    def _run_seven_agents(self, positive_set, negative_set, data_summary):
        """Run all seven agents and collect results"""
        
        agent_results = []
        
        # Agent 1: NPS Net Value
        agent_results.append(self._agent_1_nps_net_value(data_summary))
        
        # Agent 2: NPS Distribution
        agent_results.append(self._agent_2_nps_distribution(positive_set, negative_set))
        
        # Agent 3: Positive Multiple Choice
        agent_results.append(self._agent_3_positive_multiple_choice(positive_set))
        
        # Agent 4: Positive Free Answers
        agent_results.append(self._agent_4_positive_free_answers(positive_set))
        
        # Agent 5: Negative Multiple Choice
        agent_results.append(self._agent_5_negative_multiple_choice(negative_set))
        
        # Agent 6: Negative Free Answers
        agent_results.append(self._agent_6_negative_free_answers(negative_set))
        
        # Agent 7: Total Summary
        agent_results.append(self._agent_7_total_summary(agent_results[:6], data_summary))
        
        return agent_results
    
    def _agent_1_nps_net_value(self, data_summary):
        """Agent 1: AI-Powered NPS Net Value Analysis"""
        
        nps_net = data_summary["nps_net_score"]
        total_users = data_summary["total_users"]
        industry_benchmark = self.knowledge_base.get("business_rules", {}).get("nps_scoring_rules", {}).get("industry_benchmark", 30)
        key_brands = ", ".join(self.knowledge_base.get("key_brands", ["å®‰æ…•å¸Œ", "é‡‘å…¸", "èˆ’åŒ–"]))
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 1/7] Agent 1: NPSå‡€å€¼æ·±åº¦åˆ†æ...")
        
        if self.use_ai and self.ai_client:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.NPS_NET_VALUE_ANALYSIS,
                    nps_net_score=nps_net,
                    total_responses=total_users,
                    industry_benchmark=industry_benchmark,
                    key_brands=key_brands
                )
                
                # Merge AI results with basic analysis structure
                analysis = {
                    "agent_name": "NPSå‡€å€¼åˆ†ææ™ºèƒ½ä½“",
                    "question_context": self.question_texts["nps_question"],
                    "input_data": {
                        "nps_net_score": nps_net,
                        "total_responses": total_users,
                        "industry_benchmark": industry_benchmark
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å½“å‰NPSæ°´å¹³æ˜¯å¦è¾¾åˆ°è¡Œä¸šæ ‡å‡†ï¼Ÿ",
                        "éœ€è¦é‡‡å–ä»€ä¹ˆæªæ–½æå‡NPSåˆ†æ•°ï¼Ÿ",
                        "ä¸ç«äº‰å¯¹æ‰‹ç›¸æ¯”æˆ‘ä»¬çš„è¡¨ç°å¦‚ä½•ï¼Ÿ"
                    ]
                }
                
                # Extract key insights from AI response
                if "key_insights" in ai_result:
                    analysis["insight_summary"] = ai_result["key_insights"]
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "nps_level_assessment" in ai_result:
                    level_info = ai_result["nps_level_assessment"]
                    analysis["summary_text"] = f"AIåˆ†ææ˜¾ç¤ºå½“å‰NPSå‡€å€¼{nps_net:.1f}%å¤„äº{level_info.get('level', 'å¾…è¯„ä¼°')}æ°´å¹³ï¼Œåœ¨è¡Œä¸šä¸­{level_info.get('industry_comparison', 'è¡¨ç°å¾…è¯„ä¼°')}ï¼Œç«äº‰åœ°ä½{level_info.get('competitive_position', 'éœ€è¿›ä¸€æ­¥åˆ†æ')}ã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦åˆ†æäº†NPSå‡€å€¼{nps_net:.1f}%çš„è¡¨ç°ï¼Œæä¾›äº†ä¸“ä¸šçš„ä¸šåŠ¡æ´å¯Ÿå’Œæˆ˜ç•¥å»ºè®®ã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 1 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis
        return self._agent_1_fallback_analysis(data_summary)
    
    def _agent_1_fallback_analysis(self, data_summary):
        """Fallback rule-based analysis for Agent 1"""
        nps_net = data_summary["nps_net_score"]
        total_users = data_summary["total_users"]
        industry_benchmark = self.knowledge_base.get("business_rules", {}).get("nps_scoring_rules", {}).get("industry_benchmark", 30)
        
        analysis = {
            "agent_name": "NPSå‡€å€¼åˆ†ææ™ºèƒ½ä½“",
            "question_context": self.question_texts["nps_question"],
            "input_data": {
                "nps_net_score": nps_net,
                "total_responses": total_users,
                "industry_benchmark": industry_benchmark
            },
            "statements": [],
            "business_questions": [
                "å½“å‰NPSæ°´å¹³æ˜¯å¦è¾¾åˆ°è¡Œä¸šæ ‡å‡†ï¼Ÿ",
                "éœ€è¦é‡‡å–ä»€ä¹ˆæªæ–½æå‡NPSåˆ†æ•°ï¼Ÿ",
                "ä¸ç«äº‰å¯¹æ‰‹ç›¸æ¯”æˆ‘ä»¬çš„è¡¨ç°å¦‚ä½•ï¼Ÿ"
            ],
            "summary_text": "",
            "insight_summary": []
        }
        
        if nps_net >= 50:
            analysis["statements"] = [
                f"NPSå‡€å€¼ä¸º{nps_net:.1f}%ï¼Œå±äºä¼˜ç§€æ°´å¹³ï¼Œè¡¨æ˜å®¢æˆ·å¿ è¯šåº¦å¾ˆé«˜",
                "å¤§éƒ¨åˆ†å®¢æˆ·æ„¿æ„æ¨èäº§å“ï¼Œå½¢æˆäº†è‰¯å¥½çš„å£ç¢‘ä¼ æ’­",
                "å½“å‰å®¢æˆ·æ»¡æ„åº¦ç­–ç•¥æ•ˆæœæ˜¾è‘—ï¼Œåº”ç»§ç»­ä¿æŒ",
                "å¯ä»¥è€ƒè™‘æ‰©å¤§å¸‚åœºä»½é¢å’Œæå‡å“ç‰Œå½±å“åŠ›"
            ]
            analysis["summary_text"] = f"ä¼Šåˆ©é›†å›¢NPSå‡€å€¼è¾¾åˆ°{nps_net:.1f}%çš„ä¼˜ç§€æ°´å¹³ï¼Œè¿œè¶…è¡Œä¸šåŸºå‡†{industry_benchmark}%ï¼Œæ˜¾ç¤ºå‡ºå¼ºåŠ²çš„å®¢æˆ·å¿ è¯šåº¦å’Œå¸‚åœºç«äº‰åŠ›ã€‚æ¨èè€…ç¾¤ä½“å ä¸»å¯¼åœ°ä½ï¼Œä¸ºå“ç‰Œå»ºç«‹äº†è‰¯å¥½çš„å£ç¢‘ä¼ æ’­åŸºç¡€ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰ä¼˜åŠ¿ç­–ç•¥å¹¶ç§¯ææ‰©å¤§å¸‚åœºå½±å“åŠ›ã€‚"
            analysis["insight_summary"] = [
                "1. NPSè¡¨ç°ä¼˜ç§€ï¼šå‡€å€¼è¶…è¿‡50%ï¼Œå®¢æˆ·å¿ è¯šåº¦é«˜",
                "2. å£ç¢‘æ•ˆåº”æ˜¾è‘—ï¼šæ¨èè€…å½¢æˆè‰¯å¥½ä¼ æ’­",
                "3. ç­–ç•¥æ•ˆæœéªŒè¯ï¼šå½“å‰æ»¡æ„åº¦ç­–ç•¥æˆåŠŸ",
                "4. æ‰©å¼ æœºä¼šï¼šå¯è€ƒè™‘å¸‚åœºä»½é¢æå‡"
            ]
        elif nps_net >= 0:
            analysis["statements"] = [
                f"NPSå‡€å€¼ä¸º{nps_net:.1f}%ï¼Œå¤„äºä¸­ç­‰æ°´å¹³ï¼Œæœ‰æ”¹è¿›ç©ºé—´",
                "æ¨èè€…ç•¥å¤šäºè´¬æŸè€…ï¼Œä½†ä¼˜åŠ¿ä¸æ˜æ˜¾",
                "éœ€è¦é‡ç‚¹å…³æ³¨æå‡å®¢æˆ·ä½“éªŒå’Œæ»¡æ„åº¦",
                "åº”æ·±å…¥åˆ†æå®¢æˆ·ç—›ç‚¹ï¼Œåˆ¶å®šé’ˆå¯¹æ€§æ”¹è¿›æ–¹æ¡ˆ"
            ]
            analysis["summary_text"] = f"ä¼Šåˆ©é›†å›¢NPSå‡€å€¼ä¸º{nps_net:.1f}%ï¼Œå¤„äºä¸­ç­‰æ°´å¹³ï¼Œè™½ç„¶æ¨èè€…æ•°é‡ç•¥å¤šäºè´¬æŸè€…ï¼Œä½†ä¼˜åŠ¿ä¸å¤Ÿæ˜æ˜¾ï¼Œå­˜åœ¨è¾ƒå¤§æ”¹è¿›ç©ºé—´ã€‚éœ€è¦æ·±å…¥åˆ†æå®¢æˆ·ä½“éªŒç—›ç‚¹ï¼Œç‰¹åˆ«å…³æ³¨{', '.join(self.knowledge_base.get('key_brands', ['å®‰æ…•å¸Œ', 'é‡‘å…¸', 'èˆ’åŒ–'])[:3])}ç­‰æ ¸å¿ƒå“ç‰Œçš„è¡¨ç°ï¼Œåˆ¶å®šç³»ç»Ÿæ€§çš„å®¢æˆ·æ»¡æ„åº¦æå‡æ–¹æ¡ˆã€‚"
            analysis["insight_summary"] = [
                "1. NPSæ°´å¹³ä¸­ç­‰ï¼šå‡€å€¼ä¸ºæ­£ä½†ä¸å¤Ÿå¼ºåŠ²",
                "2. ä¼˜åŠ¿å¾®å¼±ï¼šæ¨èè€…ä»…ç•¥å¤šäºè´¬æŸè€…",
                "3. æ”¹è¿›ç©ºé—´å¤§ï¼šå®¢æˆ·ä½“éªŒéœ€è¦æå‡",
                "4. ç­–ç•¥è°ƒæ•´ï¼šéœ€åˆ¶å®šé’ˆå¯¹æ€§æ”¹è¿›æ–¹æ¡ˆ"
            ]
        else:
            analysis["statements"] = [
                f"NPSå‡€å€¼ä¸º{nps_net:.1f}%ï¼Œå¤„äºè´Ÿå€¼ï¼Œå®¢æˆ·æ»¡æ„åº¦äºŸéœ€æ”¹å–„",
                "è´¬æŸè€…å¤šäºæ¨èè€…ï¼Œå­˜åœ¨å®¢æˆ·æµå¤±é£é™©",
                "å¿…é¡»ç«‹å³è¯†åˆ«å¹¶è§£å†³æ ¸å¿ƒé—®é¢˜",
                "å»ºè®®æš‚ç¼“å¸‚åœºæ‰©å¼ ï¼Œä¼˜å…ˆæå‡äº§å“å’ŒæœåŠ¡è´¨é‡",
                "éœ€è¦åˆ¶å®šç´§æ€¥å®¢æˆ·æŒ½ç•™å’Œæ»¡æ„åº¦æå‡è®¡åˆ’"
            ]
            analysis["summary_text"] = f"ä¼Šåˆ©é›†å›¢NPSå‡€å€¼ä¸º{nps_net:.1f}%çš„è´Ÿå€¼è¡¨ç°ä»¤äººæ‹…å¿§ï¼Œè´¬æŸè€…æ•°é‡è¶…è¿‡æ¨èè€…ï¼Œå­˜åœ¨ä¸¥é‡çš„å®¢æˆ·æµå¤±é£é™©ã€‚å½“å‰å±€é¢è¦æ±‚ç«‹å³é‡‡å–ç´§æ€¥æªæ–½ï¼Œæš‚ç¼“å¸‚åœºæ‰©å¼ è®¡åˆ’ï¼Œé›†ä¸­èµ„æºè§£å†³äº§å“è´¨é‡ã€æœåŠ¡ä½“éªŒæˆ–ä»·æ ¼ç­‰æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶åˆ¶å®šå…¨é¢çš„å®¢æˆ·æŒ½ç•™å’Œæ»¡æ„åº¦é‡å»ºè®¡åˆ’ã€‚"
            analysis["insight_summary"] = [
                "1. NPSè¡¨ç°ä¸¥é‡ï¼šå‡€å€¼ä¸ºè´Ÿï¼Œå®¢æˆ·æ»¡æ„åº¦ä½",
                "2. æµå¤±é£é™©é«˜ï¼šè´¬æŸè€…å¤šäºæ¨èè€…",
                "3. ç´§æ€¥çŠ¶æ€ï¼šéœ€ç«‹å³è¯†åˆ«æ ¸å¿ƒé—®é¢˜",
                "4. ç­–ç•¥è°ƒæ•´ï¼šæš‚ç¼“æ‰©å¼ ï¼Œä¸“æ³¨è´¨é‡æå‡",
                "5. æŒ½ç•™è®¡åˆ’ï¼šåˆ¶å®šç´§æ€¥å®¢æˆ·æŒ½ç•™æªæ–½"
            ]
        
        return analysis
    
    def _agent_2_nps_distribution(self, positive_set, negative_set):
        """Agent 2: AI-Powered NPS Distribution Analysis"""
        
        total = len(positive_set["nps_array"]) + len(negative_set["nps_array"])
        promoters = len(positive_set["nps_array"])
        passives = len([x for x in negative_set["nps_array"] if x["nps_score"] >= 7])
        detractors = len([x for x in negative_set["nps_array"] if x["nps_score"] <= 6])
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 2/7] Agent 2: NPSåˆ†å¸ƒæ·±åº¦åˆ†æ...")
        
        if self.use_ai and self.ai_client:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.NPS_DISTRIBUTION_ANALYSIS,
                    promoters=promoters,
                    passives=passives,
                    detractors=detractors,
                    total=total,
                    promoter_pct=promoters/total*100 if total > 0 else 0,
                    passive_pct=passives/total*100 if total > 0 else 0,
                    detractor_pct=detractors/total*100 if total > 0 else 0
                )
                
                analysis = {
                    "agent_name": "NPSåˆ†å¸ƒåˆ†ææ™ºèƒ½ä½“",
                    "question_context": self.question_texts["nps_question"],
                    "input_data": {
                        "promoters": promoters,
                        "passives": passives,
                        "detractors": detractors,
                        "total": total
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å¦‚ä½•å°†ä¸­ç«‹è€…è½¬åŒ–ä¸ºæ¨èè€…ï¼Ÿ",
                        "è´¬æŸè€…çš„ä¸»è¦é—®é¢˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "æ¨èè€…ç¾¤ä½“æœ‰ä»€ä¹ˆå…±åŒç‰¹å¾ï¼Ÿ"
                    ]
                }
                
                # Extract insights from AI response
                if "business_implications" in ai_result:
                    analysis["insight_summary"] = ai_result["business_implications"]
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "distribution_health" in ai_result:
                    health_info = ai_result["distribution_health"]
                    analysis["summary_text"] = f"AIåˆ†ææ˜¾ç¤ºå®¢æˆ·åˆ†å¸ƒç»“æ„{health_info.get('structure_assessment', 'å¾…è¯„ä¼°')}ï¼Œ{health_info.get('dominant_group', 'å®¢æˆ·ç¾¤ä½“')}å ä¸»å¯¼åœ°ä½ï¼Œæ•´ä½“å¹³è¡¡è¯„åˆ†{health_info.get('balance_score', 'N/A')}ã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦åˆ†æäº†{total}ä½å®¢æˆ·çš„NPSåˆ†å¸ƒç»“æ„ï¼Œæä¾›äº†å®¢æˆ·ç¾¤ä½“ä¼˜åŒ–ç­–ç•¥å»ºè®®ã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 2 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis
        analysis = {
            "agent_name": "NPSåˆ†å¸ƒåˆ†ææ™ºèƒ½ä½“",
            "question_context": self.question_texts["nps_question"],
            "input_data": {
                "promoters": promoters,
                "passives": passives,
                "detractors": detractors,
                "total": total
            },
            "statements": [
                f"å®¢æˆ·åˆ†å¸ƒï¼šæ¨èè€…{promoters}äºº({promoters/total*100:.1f}%)ï¼Œä¸­ç«‹è€…{passives}äºº({passives/total*100:.1f}%)ï¼Œè´¬æŸè€…{detractors}äºº({detractors/total*100:.1f}%)",
                f"è´¬æŸè€…æ¯”ä¾‹{'è¾ƒé«˜' if detractors/total > 0.3 else 'é€‚ä¸­' if detractors/total > 0.1 else 'è¾ƒä½'}ï¼Œéœ€è¦{'é‡ç‚¹å…³æ³¨' if detractors/total > 0.3 else 'æŒç»­æ”¹è¿›'}",
                f"æ¨èè€…æ¯”ä¾‹{'å……è¶³' if promoters/total > 0.5 else 'æœ‰å¾…æå‡'}ï¼Œ{'å¯ä»¥' if promoters/total > 0.5 else 'éœ€è¦'}åˆ©ç”¨å£ç¢‘è¥é”€",
                f"{'æ¨èè€…' if promoters > detractors else 'è´¬æŸè€…'}ç¾¤ä½“å ä¸»å¯¼åœ°ä½ï¼Œåæ˜ äº†{'æ­£é¢' if promoters > detractors else 'è´Ÿé¢'}çš„æ•´ä½“å®¢æˆ·ä½“éªŒ"
            ],
            "business_questions": [
                "å¦‚ä½•å°†ä¸­ç«‹è€…è½¬åŒ–ä¸ºæ¨èè€…ï¼Ÿ",
                "è´¬æŸè€…çš„ä¸»è¦é—®é¢˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "æ¨èè€…ç¾¤ä½“æœ‰ä»€ä¹ˆå…±åŒç‰¹å¾ï¼Ÿ"
            ],
            "summary_text": f"ä¼Šåˆ©é›†å›¢å®¢æˆ·æ»¡æ„åº¦å‘ˆç°{promoters}äººæ¨èè€…({promoters/total*100:.1f}%)ã€{passives}äººä¸­ç«‹è€…({passives/total*100:.1f}%)ã€{detractors}äººè´¬æŸè€…({detractors/total*100:.1f}%)çš„åˆ†å¸ƒæ ¼å±€ã€‚{'æ¨èè€…' if promoters > detractors else 'è´¬æŸè€…'}ç¾¤ä½“å ä¸»å¯¼åœ°ä½ï¼Œåæ˜ å½“å‰å®¢æˆ·ä½“éªŒæ•´ä½“è¶‹å‘{'æ­£é¢' if promoters > detractors else 'è´Ÿé¢'}ã€‚ä¸­ç«‹è€…ç¾¤ä½“å­˜åœ¨è¾ƒå¤§è½¬åŒ–æ½œåŠ›ï¼Œæ˜¯æå‡NPSçš„å…³é”®çªç ´å£ã€‚",
            "insight_summary": [
                f"1. å®¢æˆ·ç»“æ„ï¼šæ¨èè€…{promoters/total*100:.1f}%ï¼Œä¸­ç«‹è€…{passives/total*100:.1f}%ï¼Œè´¬æŸè€…{detractors/total*100:.1f}%",
                f"2. ä¸»å¯¼ç¾¤ä½“ï¼š{'æ¨èè€…' if promoters > detractors else 'è´¬æŸè€…'}å ä¼˜åŠ¿åœ°ä½",
                f"3. é£é™©è¯„ä¼°ï¼šè´¬æŸè€…æ¯”ä¾‹{'è¾ƒé«˜' if detractors/total > 0.3 else 'é€‚ä¸­' if detractors/total > 0.1 else 'è¾ƒä½'}",
                f"4. è½¬åŒ–æœºä¼šï¼š{passives}ä½ä¸­ç«‹è€…å…·å¤‡æå‡æ½œåŠ›"
            ]
        }
        
        return analysis
    
    def _agent_3_positive_multiple_choice(self, positive_set):
        """Agent 3: AI-Powered Positive Multiple Choice Analysis"""
        
        # Flatten positive factors
        all_positive_factors = []
        for item in positive_set["multiple_choice_array"]:
            all_positive_factors.extend(item["positive_factors"])
        
        # Count factor frequency
        factor_counts = {}
        for factor in all_positive_factors:
            factor_counts[factor] = factor_counts.get(factor, 0) + 1
        
        flattened_text = " | ".join(all_positive_factors)
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 3/7] Agent 3: æ­£é¢å› ç´ æ·±åº¦åˆ†æ...")
        
        if self.use_ai and self.ai_client and all_positive_factors:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.POSITIVE_FACTORS_ANALYSIS,
                    total_selections=len(all_positive_factors),
                    unique_factors=len(factor_counts),
                    factor_frequency=factor_counts,
                    flattened_text=flattened_text
                )
                
                analysis = {
                    "agent_name": "æ­£é¢å¤šé€‰é¢˜åˆ†ææ™ºèƒ½ä½“",
                    "question_context": self.question_texts["positive_factors"],
                    "input_data": {
                        "total_selections": len(all_positive_factors),
                        "unique_factors": len(factor_counts),
                        "flattened_text": flattened_text,
                        "factor_frequency": factor_counts
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å“ªäº›æ­£é¢å› ç´ æœ€èƒ½æ‰“åŠ¨å®¢æˆ·ï¼Ÿ",
                        "å¦‚ä½•åœ¨è¥é”€ä¸­å¼ºåŒ–è¿™äº›ä¼˜åŠ¿ï¼Ÿ",
                        "è¿™äº›å› ç´ å¦‚ä½•è½¬åŒ–ä¸ºç«äº‰ä¼˜åŠ¿ï¼Ÿ"
                    ]
                }
                
                # Extract insights from AI response
                if "marketing_opportunities" in ai_result:
                    insights = []
                    for opp in ai_result["marketing_opportunities"]:
                        insights.append(f"è¥é”€æœºä¼š: {opp.get('advantage', 'N/A')} - {opp.get('communication_message', 'N/A')}")
                    analysis["insight_summary"] = insights
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "core_advantages" in ai_result:
                    advantage_info = ai_result["core_advantages"]
                    analysis["summary_text"] = f"AIåˆ†æè¯†åˆ«å‡ºæ ¸å¿ƒä¼˜åŠ¿'{advantage_info.get('primary_strength', 'N/A')}'ï¼Œä¸ºè¥é”€ä¼ æ’­å’Œç«äº‰å®šä½æä¾›äº†æ˜ç¡®æ–¹å‘ã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦åˆ†æäº†{len(all_positive_factors)}ä¸ªæ­£é¢å› ç´ ï¼Œæä¾›äº†è¥é”€æœºä¼šå’Œç«äº‰ä¼˜åŠ¿å»ºè®®ã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 3 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis or handle empty data
        factor_categories = self._categorize_factors(all_positive_factors, "positive")
        
        analysis = {
            "agent_name": "æ­£é¢å¤šé€‰é¢˜åˆ†ææ™ºèƒ½ä½“",
            "question_context": self.question_texts["positive_factors"],
            "input_data": {
                "total_selections": len(all_positive_factors),
                "unique_factors": len(factor_counts),
                "flattened_text": flattened_text,
                "factor_frequency": factor_counts,
                "factor_categories": factor_categories
            },
            "statements": [],
            "business_questions": [
                "å“ªäº›æ­£é¢å› ç´ æœ€èƒ½æ‰“åŠ¨å®¢æˆ·ï¼Ÿ",
                "å¦‚ä½•åœ¨è¥é”€ä¸­å¼ºåŒ–è¿™äº›ä¼˜åŠ¿ï¼Ÿ",
                "è¿™äº›å› ç´ å¦‚ä½•è½¬åŒ–ä¸ºç«äº‰ä¼˜åŠ¿ï¼Ÿ"
            ],
            "summary_text": "",
            "insight_summary": []
        }
        
        if not all_positive_factors:
            analysis["statements"] = [
                "æ¨èè€…ç¾¤ä½“æœªæä¾›å…·ä½“çš„æ­£é¢å› ç´ é€‰æ‹©",
                "éœ€è¦é€šè¿‡æ·±åº¦è®¿è°ˆäº†è§£æ¨èçš„çœŸå®åŸå› ",
                "å»ºè®®ä¼˜åŒ–é—®å·è®¾è®¡ï¼Œå¢åŠ æ›´è´´è¿‘å®¢æˆ·ä½“éªŒçš„é€‰é¡¹"
            ]
            analysis["summary_text"] = "æ¨èè€…ç¾¤ä½“åœ¨å¤šé€‰é¢˜ä¸­ç¼ºä¹å…·ä½“çš„æ­£é¢å› ç´ åé¦ˆï¼Œè¡¨æ˜é—®å·è®¾è®¡å¯èƒ½ä¸å¤Ÿè´´è¿‘å®é™…å®¢æˆ·ä½“éªŒï¼Œæˆ–æ¨èåŠ¨æœºè¾ƒä¸ºå¤æ‚éš¾ä»¥é€šè¿‡æ ‡å‡†é€‰é¡¹è¡¨è¾¾ã€‚å»ºè®®é€šè¿‡å®šæ€§ç ”ç©¶æ–¹æ³•æ·±å…¥äº†è§£å®¢æˆ·çœŸå®çš„æ¨èé©±åŠ¨å› ç´ ã€‚"
            analysis["insight_summary"] = [
                "1. æ•°æ®ç¼ºå¤±ï¼šæ¨èè€…æœªé€‰æ‹©å…·ä½“æ­£é¢å› ç´ ",
                "2. è®¾è®¡é—®é¢˜ï¼šé—®å·é€‰é¡¹å¯èƒ½ä¸å¤Ÿè´´åˆ‡",
                "3. ç ”ç©¶å»ºè®®ï¼šéœ€è¦æ·±åº¦è®¿è°ˆè¡¥å……"
            ]
        else:
            top_factors = sorted(factor_counts.items(), key=lambda x: x[1], reverse=True)
            main_category = max(factor_categories.items(), key=lambda x: x[1]) if factor_categories else ("äº§å“å“è´¨", 0)
            
            analysis["statements"] = [
                f"æ¨èè€…å…±é€‰æ‹©äº†{len(all_positive_factors)}ä¸ªæ­£é¢å› ç´ ï¼Œæ¶‰åŠ{len(factor_counts)}ä¸ªä¸åŒç»´åº¦",
                f"æœ€å—è®¤å¯çš„å› ç´ æ˜¯'{top_factors[0][0]}'ï¼Œè¢«{top_factors[0][1]}ä½å®¢æˆ·é€‰æ‹©",
                f"æ­£é¢å› ç´ ä¸»è¦é›†ä¸­åœ¨{main_category[0]}æ–¹é¢ï¼Œå æ¯”{main_category[1]/len(all_positive_factors)*100:.1f}%",
                "è¿™äº›ä¼˜åŠ¿å› ç´ å¯ä»¥ä½œä¸ºå“ç‰Œä¼ æ’­å’Œäº§å“æ”¹è¿›çš„é‡ç‚¹æ–¹å‘"
            ]
            analysis["summary_text"] = f"æ¨èè€…ç¾¤ä½“é€‰æ‹©äº†{len(all_positive_factors)}ä¸ªæ­£é¢å› ç´ ï¼Œä¸»è¦é›†ä¸­åœ¨{main_category[0]}é¢†åŸŸã€‚æœ€å—è®¤å¯çš„'{top_factors[0][0]}'è¢«{top_factors[0][1]}ä½å®¢æˆ·é€‰æ‹©ï¼Œåæ˜ äº†ä¼Šåˆ©åœ¨{main_category[0]}æ–¹é¢çš„å¸‚åœºä¼˜åŠ¿ã€‚è¿™äº›ä¼˜åŠ¿å› ç´ ä¸º{', '.join(self.knowledge_base.get('key_brands', ['å®‰æ…•å¸Œ', 'é‡‘å…¸'])[:2])}ç­‰æ ¸å¿ƒå“ç‰Œçš„è¥é”€ä¼ æ’­å’Œäº§å“ä¼˜åŒ–æä¾›äº†æ˜ç¡®æ–¹å‘ã€‚"
            analysis["insight_summary"] = [
                f"1. é€‰æ‹©é›†ä¸­åº¦ï¼š{len(all_positive_factors)}ä¸ªå› ç´ æ¶‰åŠ{len(factor_counts)}ä¸ªç»´åº¦",
                f"2. æ ¸å¿ƒä¼˜åŠ¿ï¼š'{top_factors[0][0]}'æœ€å—è®¤å¯",
                f"3. ä¸»è¦é¢†åŸŸï¼š{main_category[0]}å æ¯”{main_category[1]/len(all_positive_factors)*100:.1f}%",
                "4. è¥é”€ä»·å€¼ï¼šä¼˜åŠ¿å› ç´ æŒ‡å¯¼å“ç‰Œä¼ æ’­æ–¹å‘"
            ]
        
        return analysis
    
    def _agent_4_positive_free_answers(self, positive_set):
        """Agent 4: AI-Powered Positive Free Answers Analysis"""
        
        # Flatten positive reasons
        positive_reasons = []
        for item in positive_set["free_question_array"]:
            if item["positive_reason"].strip():
                positive_reasons.append(item["positive_reason"])
        
        flattened_text = " | ".join(positive_reasons)
        response_rate = len(positive_reasons) / len(positive_set["free_question_array"]) * 100 if positive_set["free_question_array"] else 0
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 4/7] Agent 4: æ­£é¢å¼€æ”¾å›ç­”æ·±åº¦åˆ†æ...")
        
        if self.use_ai and self.ai_client and positive_reasons:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.POSITIVE_OPENTEXT_ANALYSIS,
                    total_responses=len(positive_reasons),
                    response_rate=response_rate,
                    flattened_text=flattened_text
                )
                
                analysis = {
                    "agent_name": "æ­£é¢å¡«ç©ºé¢˜åˆ†ææ™ºèƒ½ä½“",
                    "question_context": self.question_texts["positive_specific"],
                    "input_data": {
                        "total_responses": len(positive_reasons),
                        "flattened_text": flattened_text,
                        "response_rate": response_rate
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å®¢æˆ·æœ€çœŸå®çš„æ¨èç†ç”±æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "è¿™äº›è‡ªå‘è¡¨è¾¾åæ˜ äº†ä»€ä¹ˆæ·±å±‚éœ€æ±‚ï¼Ÿ",
                        "å¦‚ä½•å°†è¿™äº›æ´å¯Ÿè½¬åŒ–ä¸ºäº§å“ä¼˜åŒ–ï¼Ÿ"
                    ]
                }
                
                # Extract insights from AI response
                if "customer_stories" in ai_result:
                    insights = []
                    for story in ai_result["customer_stories"]:
                        insights.append(f"å®¢æˆ·æ•…äº‹: {story.get('story_theme', 'N/A')} - {story.get('marketing_value', 'N/A')}")
                    analysis["insight_summary"] = insights
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "emotional_insights" in ai_result:
                    emotion_info = ai_result["emotional_insights"]
                    connection_strength = emotion_info.get("emotional_connection_strength", "N/A")
                    analysis["summary_text"] = f"AIåˆ†ææ˜¾ç¤ºå®¢æˆ·æƒ…æ„Ÿè¿æ¥å¼ºåº¦ä¸º{connection_strength}ï¼Œè¯†åˆ«å‡ºæœ‰ä»·å€¼çš„å®¢æˆ·æ•…äº‹å’Œä½¿ç”¨åœºæ™¯æ´å¯Ÿã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦åˆ†æäº†{len(positive_reasons)}æ¡æ¨èç†ç”±ï¼Œæ­ç¤ºäº†å®¢æˆ·çš„çœŸå®æƒ…æ„Ÿå’Œæ·±å±‚éœ€æ±‚ã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 4 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis
        analysis = {
            "agent_name": "æ­£é¢å¡«ç©ºé¢˜åˆ†ææ™ºèƒ½ä½“",
            "question_context": self.question_texts["positive_specific"],
            "input_data": {
                "total_responses": len(positive_reasons),
                "flattened_text": flattened_text,
                "response_rate": response_rate
            },
            "statements": [],
            "business_questions": [
                "å®¢æˆ·æœ€çœŸå®çš„æ¨èç†ç”±æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¿™äº›è‡ªå‘è¡¨è¾¾åæ˜ äº†ä»€ä¹ˆæ·±å±‚éœ€æ±‚ï¼Ÿ",
                "å¦‚ä½•å°†è¿™äº›æ´å¯Ÿè½¬åŒ–ä¸ºäº§å“ä¼˜åŒ–ï¼Ÿ"
            ]
        }
        
        if not positive_reasons:
            analysis["statements"] = [
                "æ¨èè€…æœªæä¾›å…·ä½“çš„æ¨èç†ç”±æè¿°",
                "ç¼ºä¹æ·±åº¦çš„å®¢æˆ·æ´å¯Ÿå’Œæƒ…æ„Ÿè¿æ¥ä¿¡æ¯",
                "å»ºè®®å¢åŠ å¼€æ”¾å¼é—®é¢˜å¼•å¯¼å®¢æˆ·åˆ†äº«çœŸå®æ„Ÿå—"
            ]
            analysis["summary_text"] = "æ¨èè€…æœªæä¾›å…·ä½“çš„æ¨èç†ç”±æè¿°ï¼Œç¼ºä¹æ·±åº¦çš„å®¢æˆ·æ´å¯Ÿã€‚"
            analysis["insight_summary"] = ["æ•°æ®ç¼ºå¤±ï¼šæ¨èè€…æœªæä¾›å¼€æ”¾å›ç­”"]
        else:
            # Analyze content themes
            themes = []
            combined_text = " ".join(positive_reasons)
            if "å£å‘³" in combined_text or "å¥½å–" in combined_text:
                themes.append("äº§å“å£æ„Ÿ")
            if "åŒ…è£…" in combined_text or "å¤–è§‚" in combined_text:
                themes.append("åŒ…è£…è®¾è®¡")
            if "é€‰æ‹©" in combined_text or "å¤šæ ·" in combined_text:
                themes.append("äº§å“å¤šæ ·æ€§")
            
            analysis["statements"] = [
                f"æ¨èè€…æä¾›äº†{len(positive_reasons)}æ¡å…·ä½“æ¨èç†ç”±ï¼Œå›ç­”ç‡{response_rate:.1f}%",
                f"å®¢æˆ·è¡¨è¾¾é‡ç‚¹å…³æ³¨{'ã€'.join(themes) if themes else 'æ•´ä½“ä½“éªŒ'}",
                "æ¨èç†ç”±ä½“ç°äº†å®¢æˆ·çš„çœŸå®ä½¿ç”¨æ„Ÿå—å’Œæƒ…æ„Ÿè¿æ¥",
                "è¿™äº›è‡ªå‘è¡¨è¾¾ä¸ºäº§å“æ”¹è¿›å’Œè¥é”€ä¼ æ’­æä¾›äº†å®è´µæ´å¯Ÿ"
            ]
            analysis["summary_text"] = f"æ¨èè€…æä¾›äº†{len(positive_reasons)}æ¡å…·ä½“æ¨èç†ç”±ï¼Œå®¢æˆ·è¡¨è¾¾é‡ç‚¹å…³æ³¨{'ã€'.join(themes) if themes else 'æ•´ä½“ä½“éªŒ'}ã€‚"
            analysis["insight_summary"] = [
                f"1. å›ç­”æƒ…å†µï¼š{len(positive_reasons)}æ¡ç†ç”±ï¼Œå›ç­”ç‡{response_rate:.1f}%",
                f"2. å…³æ³¨é‡ç‚¹ï¼š{'ã€'.join(themes) if themes else 'æ•´ä½“ä½“éªŒ'}",
                "3. æƒ…æ„Ÿè¿æ¥ï¼šä½“ç°å®¢æˆ·çœŸå®ä½¿ç”¨æ„Ÿå—",
                "4. æ´å¯Ÿä»·å€¼ï¼šä¸ºäº§å“æ”¹è¿›æä¾›æ–¹å‘"
            ]
        
        return analysis
    
    def _agent_5_negative_multiple_choice(self, negative_set):
        """Agent 5: AI-Powered Negative Multiple Choice Analysis"""
        
        # Flatten negative factors
        all_negative_factors = []
        for item in negative_set["multiple_choice_array"]:
            all_negative_factors.extend(item["negative_factors"])
        
        # Count factor frequency
        factor_counts = {}
        for factor in all_negative_factors:
            factor_counts[factor] = factor_counts.get(factor, 0) + 1
        
        flattened_text = " | ".join(all_negative_factors)
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 5/7] Agent 5: è´Ÿé¢å› ç´ æ·±åº¦åˆ†æ...")
        
        if self.use_ai and self.ai_client and all_negative_factors:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.NEGATIVE_FACTORS_ANALYSIS,
                    total_selections=len(all_negative_factors),
                    unique_factors=len(factor_counts),
                    factor_frequency=factor_counts,
                    flattened_text=flattened_text
                )
                
                analysis = {
                    "agent_name": "è´Ÿé¢å¤šé€‰é¢˜åˆ†ææ™ºèƒ½ä½“",
                    "question_context": self.question_texts["negative_factors"],
                    "input_data": {
                        "total_selections": len(all_negative_factors),
                        "unique_factors": len(factor_counts),
                        "flattened_text": flattened_text,
                        "factor_frequency": factor_counts
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å“ªäº›é—®é¢˜æ˜¯å®¢æˆ·ä¸æ¨èçš„ä¸»è¦åŸå› ï¼Ÿ",
                        "è¿™äº›é—®é¢˜çš„è§£å†³ä¼˜å…ˆçº§å¦‚ä½•æ’åºï¼Ÿ",
                        "éœ€è¦æŠ•å…¥å¤šå°‘èµ„æºæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Ÿ"
                    ]
                }
                
                # Extract insights from AI response
                if "solution_roadmap" in ai_result:
                    insights = []
                    for solution in ai_result["solution_roadmap"]:
                        insights.append(f"è§£å†³æ–¹æ¡ˆ: {solution.get('problem', 'N/A')} - ä¼˜å…ˆçº§: {solution.get('priority', 'N/A')}")
                    analysis["insight_summary"] = insights
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "critical_issues" in ai_result:
                    issue_info = ai_result["critical_issues"]
                    primary_problem = issue_info.get("primary_problem", "N/A")
                    analysis["summary_text"] = f"AIåˆ†æè¯†åˆ«å‡ºæ ¸å¿ƒé—®é¢˜'{primary_problem}'ï¼Œæä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³è·¯çº¿å›¾å’Œé£é™©è¯„ä¼°ã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦åˆ†æäº†{len(all_negative_factors)}ä¸ªè´Ÿé¢å› ç´ ï¼Œåˆ¶å®šäº†é—®é¢˜è§£å†³çš„ä¼˜å…ˆçº§å’Œè¡ŒåŠ¨æ–¹æ¡ˆã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 5 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis
        analysis = {
            "agent_name": "è´Ÿé¢å¤šé€‰é¢˜åˆ†ææ™ºèƒ½ä½“",
            "question_context": self.question_texts["negative_factors"],
            "input_data": {
                "total_selections": len(all_negative_factors),
                "unique_factors": len(factor_counts),
                "flattened_text": flattened_text,
                "factor_frequency": factor_counts
            },
            "statements": [],
            "business_questions": [
                "å“ªäº›é—®é¢˜æ˜¯å®¢æˆ·ä¸æ¨èçš„ä¸»è¦åŸå› ï¼Ÿ",
                "è¿™äº›é—®é¢˜çš„è§£å†³ä¼˜å…ˆçº§å¦‚ä½•æ’åºï¼Ÿ",
                "éœ€è¦æŠ•å…¥å¤šå°‘èµ„æºæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Ÿ"
            ]
        }
        
        if not all_negative_factors:
            analysis["statements"] = [
                "éæ¨èè€…ç¾¤ä½“æœªæ˜ç¡®æŒ‡å‡ºå…·ä½“çš„è´Ÿé¢å› ç´ ",
                "å¯èƒ½å­˜åœ¨é—®é¢˜è¯†åˆ«ä¸å……åˆ†æˆ–é—®å·é€‰é¡¹ä¸å¤Ÿè´´åˆ‡çš„æƒ…å†µ",
                "å»ºè®®é€šè¿‡å®šæ€§ç ”ç©¶æ·±å…¥äº†è§£å®¢æˆ·ä¸æ»¡çš„æ ¹æœ¬åŸå› "
            ]
            analysis["summary_text"] = "éæ¨èè€…ç¾¤ä½“æœªæ˜ç¡®æŒ‡å‡ºå…·ä½“çš„è´Ÿé¢å› ç´ ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶ã€‚"
            analysis["insight_summary"] = ["æ•°æ®ç¼ºå¤±ï¼šéæ¨èè€…æœªé€‰æ‹©å…·ä½“è´Ÿé¢å› ç´ "]
        else:
            top_factors = sorted(factor_counts.items(), key=lambda x: x[1], reverse=True)
            analysis["statements"] = [
                f"éæ¨èè€…è¯†åˆ«äº†{len(all_negative_factors)}ä¸ªè´Ÿé¢å› ç´ ï¼Œæ¶‰åŠ{len(factor_counts)}ä¸ªä¸åŒé—®é¢˜é¢†åŸŸ",
                f"æœ€çªå‡ºçš„é—®é¢˜æ˜¯'{top_factors[0][0]}'ï¼Œè¢«{top_factors[0][1]}ä½å®¢æˆ·æåŠ",
                f"è´Ÿé¢å› ç´ ä¸»è¦é›†ä¸­åœ¨{'è¥é”€å®£ä¼ ' if any('å®£ä¼ ' in f or 'å“ç‰Œ' in f for f in all_negative_factors) else 'äº§å“å“è´¨' if any('å£å‘³' in f or 'è´¨é‡' in f for f in all_negative_factors) else 'æœåŠ¡ä½“éªŒ'}æ–¹é¢",
                "è¿™äº›é—®é¢˜ç‚¹éœ€è¦ç«‹å³åˆ¶å®šæ”¹è¿›æ–¹æ¡ˆï¼Œé˜²æ­¢å®¢æˆ·æµå¤±"
            ]
            analysis["summary_text"] = f"éæ¨èè€…è¯†åˆ«äº†{len(all_negative_factors)}ä¸ªè´Ÿé¢å› ç´ ï¼Œæœ€çªå‡ºçš„é—®é¢˜æ˜¯'{top_factors[0][0]}'ã€‚"
            analysis["insight_summary"] = [
                f"1. é—®é¢˜è§„æ¨¡ï¼š{len(all_negative_factors)}ä¸ªå› ç´ æ¶‰åŠ{len(factor_counts)}ä¸ªé¢†åŸŸ",
                f"2. æ ¸å¿ƒé—®é¢˜ï¼š'{top_factors[0][0]}'è¢«{top_factors[0][1]}ä½å®¢æˆ·æåŠ",
                "3. æ”¹è¿›ä¼˜å…ˆçº§ï¼šéœ€è¦ç«‹å³åˆ¶å®šè§£å†³æ–¹æ¡ˆ",
                "4. æµå¤±é£é™©ï¼šé˜²æ­¢å®¢æˆ·è¿›ä¸€æ­¥æµå¤±"
            ]
        
        return analysis
    
    def _agent_6_negative_free_answers(self, negative_set):
        """Agent 6: AI-Powered Negative Free Answers Analysis"""
        
        # Flatten negative reasons
        negative_reasons = []
        for item in negative_set["free_question_array"]:
            if item["negative_reason"].strip():
                negative_reasons.append(item["negative_reason"])
        
        flattened_text = " | ".join(negative_reasons)
        response_rate = len(negative_reasons) / len(negative_set["free_question_array"]) * 100 if negative_set["free_question_array"] else 0
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 6/7] Agent 6: è´Ÿé¢å¼€æ”¾å›ç­”æ·±åº¦åˆ†æ...")
        
        if self.use_ai and self.ai_client and negative_reasons:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.NEGATIVE_OPENTEXT_ANALYSIS,
                    total_responses=len(negative_reasons),
                    response_rate=response_rate,
                    flattened_text=flattened_text
                )
                
                analysis = {
                    "agent_name": "è´Ÿé¢å¡«ç©ºé¢˜åˆ†ææ™ºèƒ½ä½“",
                    "question_context": self.question_texts["negative_specific"],
                    "input_data": {
                        "total_responses": len(negative_reasons),
                        "flattened_text": flattened_text,
                        "response_rate": response_rate
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å®¢æˆ·æœ€ä¸æ»¡æ„çš„å…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "è¿™äº›é—®é¢˜åæ˜ äº†å“ªäº›ç³»ç»Ÿæ€§ç¼ºé™·ï¼Ÿ",
                        "å¦‚ä½•åˆ¶å®šæœ‰æ•ˆçš„é—®é¢˜è§£å†³æ–¹æ¡ˆï¼Ÿ"
                    ]
                }
                
                # Extract insights from AI response
                if "improvement_priorities" in ai_result:
                    insights = []
                    for priority in ai_result["improvement_priorities"]:
                        insights.append(f"æ”¹è¿›ä¼˜å…ˆçº§: {priority.get('area', 'N/A')} - å½±å“ç¨‹åº¦: {priority.get('impact_level', 'N/A')}")
                    analysis["insight_summary"] = insights
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "dissatisfaction_analysis" in ai_result:
                    dissatisfaction_info = ai_result["dissatisfaction_analysis"]
                    emotional_intensity = dissatisfaction_info.get("emotional_intensity", "N/A")
                    analysis["summary_text"] = f"AIåˆ†ææ˜¾ç¤ºå®¢æˆ·ä¸æ»¡æƒ…ç»ªå¼ºåº¦ä¸º{emotional_intensity}ï¼Œè¯†åˆ«å‡ºå…³é”®ç—›ç‚¹å’Œç³»ç»Ÿæ€§æ”¹è¿›æ–¹å‘ã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦åˆ†æäº†{len(negative_reasons)}æ¡ä¸æ»¡åŸå› ï¼Œæä¾›äº†å®¢æˆ·æŒ½ç•™ç­–ç•¥å’Œæ”¹è¿›ä¼˜å…ˆçº§å»ºè®®ã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 6 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis
        analysis = {
            "agent_name": "è´Ÿé¢å¡«ç©ºé¢˜åˆ†ææ™ºèƒ½ä½“",
            "question_context": self.question_texts["negative_specific"],
            "input_data": {
                "total_responses": len(negative_reasons),
                "flattened_text": flattened_text,
                "response_rate": response_rate
            },
            "statements": [],
            "business_questions": [
                "å®¢æˆ·æœ€ä¸æ»¡æ„çš„å…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¿™äº›é—®é¢˜åæ˜ äº†å“ªäº›ç³»ç»Ÿæ€§ç¼ºé™·ï¼Ÿ",
                "å¦‚ä½•åˆ¶å®šæœ‰æ•ˆçš„é—®é¢˜è§£å†³æ–¹æ¡ˆï¼Ÿ"
            ]
        }
        
        if not negative_reasons:
            analysis["statements"] = [
                "éæ¨èè€…æœªè¯¦ç»†è¯´æ˜ä¸æ¨èçš„å…·ä½“åŸå› ",
                "ç¼ºä¹æ·±åº¦çš„é—®é¢˜æ´å¯Ÿï¼Œéš¾ä»¥åˆ¶å®šé’ˆå¯¹æ€§æ”¹è¿›æªæ–½",
                "å»ºè®®åŠ å¼ºé—®é¢˜æŒ–æ˜ï¼Œäº†è§£å®¢æˆ·çœŸå®ç—›ç‚¹"
            ]
            analysis["summary_text"] = "éæ¨èè€…æœªè¯¦ç»†è¯´æ˜ä¸æ¨èçš„å…·ä½“åŸå› ï¼Œç¼ºä¹æ·±åº¦æ´å¯Ÿã€‚"
            analysis["insight_summary"] = ["æ•°æ®ç¼ºå¤±ï¼šéæ¨èè€…æœªæä¾›å¼€æ”¾å›ç­”"]
        else:
            # Analyze problem themes
            themes = []
            combined_text = " ".join(negative_reasons)
            if "å®£ä¼ " in combined_text or "ä»£è¨€" in combined_text:
                themes.append("è¥é”€å®£ä¼ é—®é¢˜")
            if "æ‰¾åˆ°" in combined_text or "ç½‘é¡µ" in combined_text:
                themes.append("è´­ä¹°ä¾¿åˆ©æ€§")
            if "åŒ…è£…" in combined_text:
                themes.append("åŒ…è£…è®¾è®¡")
            
            analysis["statements"] = [
                f"éæ¨èè€…æä¾›äº†{len(negative_reasons)}æ¡å…·ä½“ä¸æ»¡åŸå› ï¼Œå›ç­”ç‡{response_rate:.1f}%",
                f"å®¢æˆ·æŠ±æ€¨ä¸»è¦é›†ä¸­åœ¨{'ã€'.join(themes) if themes else 'æ•´ä½“ä½“éªŒ'}æ–¹é¢",
                "è¿™äº›å…·ä½“é—®é¢˜åæ˜ äº†å®¢æˆ·ä½“éªŒä¸­çš„å…³é”®ç—›ç‚¹",
                "éœ€è¦è·¨éƒ¨é—¨åä½œè§£å†³è¿™äº›æ ¹æœ¬æ€§é—®é¢˜"
            ]
            analysis["summary_text"] = f"éæ¨èè€…æä¾›äº†{len(negative_reasons)}æ¡å…·ä½“ä¸æ»¡åŸå› ï¼Œä¸»è¦é›†ä¸­åœ¨{'ã€'.join(themes) if themes else 'æ•´ä½“ä½“éªŒ'}æ–¹é¢ã€‚"
            analysis["insight_summary"] = [
                f"1. å›ç­”æƒ…å†µï¼š{len(negative_reasons)}æ¡åŸå› ï¼Œå›ç­”ç‡{response_rate:.1f}%",
                f"2. é—®é¢˜ç„¦ç‚¹ï¼š{'ã€'.join(themes) if themes else 'æ•´ä½“ä½“éªŒ'}",
                "3. ç—›ç‚¹åæ˜ ï¼šå®¢æˆ·ä½“éªŒå…³é”®é—®é¢˜",
                "4. è§£å†³æ–¹å‘ï¼šéœ€è¦è·¨éƒ¨é—¨åä½œ"
            ]
        
        return analysis
    
    def _agent_7_total_summary(self, all_agent_results, data_summary):
        """Agent 7: AI-Powered Comprehensive Summary Analysis"""
        
        # Collect summaries from all previous agents
        agent_summaries = {}
        for i, result in enumerate(all_agent_results, 1):
            if "ai_analysis" in result:
                # Extract key insights from AI analysis
                ai_result = result["ai_analysis"]
                if isinstance(ai_result, dict):
                    key_info = str(ai_result.get("key_insights", ai_result.get("analysis_text", "AIåˆ†æç»“æœ")))
                else:
                    key_info = str(ai_result)
            else:
                # Use summary_text or statements from fallback analysis
                key_info = result.get("summary_text", " ".join(result.get("statements", [])))
            
            agent_summaries[f"agent{i}_summary"] = key_info[:300]  # Limit length for prompt
        
        key_brands = ", ".join(self.knowledge_base.get("key_brands", ["å®‰æ…•å¸Œ", "é‡‘å…¸", "èˆ’åŒ–"]))
        
        logger.info("ğŸ¤– [AIè°ƒç”¨ 7/7] Agent 7: ç»¼åˆæˆ˜ç•¥åˆ†æ...")
        
        if self.use_ai and self.ai_client:
            try:
                ai_result = self.ai_client.analyze_with_prompt(
                    YiliPromptTemplates.COMPREHENSIVE_SUMMARY_ANALYSIS,
                    **agent_summaries,
                    key_brands=key_brands
                )
                
                analysis = {
                    "agent_name": "ç»¼åˆæ€»ç»“åˆ†ææ™ºèƒ½ä½“",
                    "question_context": "ç»¼åˆæ‰€æœ‰é—®é¢˜çš„åˆ†æç»“æœ",
                    "input_data": {
                        "total_insights": sum(len(result.get("insight_summary", [])) for result in all_agent_results),
                        "agent_count": len(all_agent_results),
                        "agent_summaries": agent_summaries
                    },
                    "ai_analysis": ai_result,
                    "business_questions": [
                        "å¦‚ä½•åˆ¶å®šç»¼åˆæ€§çš„NPSæå‡æˆ˜ç•¥ï¼Ÿ",
                        "å„éƒ¨é—¨åº”è¯¥å¦‚ä½•åä½œæ”¹å–„å®¢æˆ·ä½“éªŒï¼Ÿ",
                        "å¦‚ä½•å¹³è¡¡çŸ­æœŸæ”¹è¿›å’Œé•¿æœŸå“ç‰Œå»ºè®¾ï¼Ÿ",
                        "éœ€è¦è®¾ç«‹ä»€ä¹ˆKPIæ¥è·Ÿè¸ªæ”¹è¿›æ•ˆæœï¼Ÿ"
                    ],
                    "additional_info": self.additional_info
                }
                
                # Extract insights from AI response
                if "executive_summary" in ai_result:
                    exec_summary = ai_result["executive_summary"]
                    analysis["insight_summary"] = [
                        f"æˆ˜ç•¥æ´å¯Ÿ: {exec_summary.get('key_insights', 'N/A')}",
                        f"æ ¸å¿ƒå»ºè®®: {exec_summary.get('recommendations', 'N/A')}",
                        f"é¢„æœŸå½±å“: {exec_summary.get('expected_impact', 'N/A')}"
                    ]
                else:
                    analysis["insight_summary"] = ai_result.get("key_insights", ["AIç»¼åˆåˆ†æå®Œæˆï¼Œè¯¦è§AIåˆ†æç»“æœ"])
                
                # Generate summary from AI analysis
                if "comprehensive_assessment" in ai_result:
                    assessment = ai_result["comprehensive_assessment"]
                    overall_health = assessment.get("overall_nps_health", "N/A")
                    key_drivers = ", ".join(assessment.get("key_drivers", [])[:2])
                    analysis["summary_text"] = f"AIç»¼åˆåˆ†ææ˜¾ç¤ºNPSæ•´ä½“å¥åº·åº¦ä¸º{overall_health}ï¼Œæ ¸å¿ƒé©±åŠ¨å› ç´ åŒ…æ‹¬{key_drivers}ï¼Œæä¾›äº†ç³»ç»Ÿæ€§çš„æˆ˜ç•¥è·¯çº¿å›¾å’Œè·¨éƒ¨é—¨åä½œå»ºè®®ã€‚"
                else:
                    analysis["summary_text"] = f"AIæ·±åº¦æ•´åˆäº†{len(all_agent_results)}ä¸ªæ™ºèƒ½ä½“çš„åˆ†æç»“æœï¼Œå½¢æˆäº†å…¨é¢çš„NPSæå‡æˆ˜ç•¥å’Œè¡ŒåŠ¨è®¡åˆ’ã€‚"
                
                return analysis
                
            except Exception as e:
                logger.warning(f"Agent 7 AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {e}")
        
        # Fallback to rule-based analysis
        all_statements = []
        for result in all_agent_results:
            all_statements.extend(result.get("statements", []))
        
        combined_insights = " ".join(all_statements)
        
        analysis = {
            "agent_name": "ç»¼åˆæ€»ç»“åˆ†ææ™ºèƒ½ä½“",
            "question_context": "ç»¼åˆæ‰€æœ‰é—®é¢˜çš„åˆ†æç»“æœ",
            "input_data": {
                "total_insights": len(all_statements),
                "agent_count": len(all_agent_results),
                "combined_insights": combined_insights
            },
            "statements": [
                f"åŸºäº{len(all_agent_results)}ä¸ªç»´åº¦çš„æ·±åº¦åˆ†æï¼Œè¯†åˆ«å‡ºå½“å‰NPSè¡¨ç°çš„å…³é”®ç‰¹å¾",
                "æ¨èè€…ç¾¤ä½“å±•ç°å‡ºå¯¹äº§å“å“è´¨å’Œå¤šæ ·æ€§çš„è®¤å¯ï¼Œä½†åŸºæ•°ä»éœ€æ‰©å¤§",
                "éæ¨èè€…ä¸»è¦å…³æ³¨è¥é”€å®£ä¼ å’Œäº§å“å‘ç°ä¾¿åˆ©æ€§é—®é¢˜ï¼Œåæ˜ äº†å“ç‰Œä¼ æ’­å’Œæ¸ é“ä¼˜åŒ–çš„éœ€æ±‚",
                "å®¢æˆ·åé¦ˆå‘ˆç°ä¸¤æåˆ†åŒ–è¶‹åŠ¿ï¼Œéœ€è¦å¹³è¡¡ä¸åŒå®¢æˆ·ç¾¤ä½“çš„éœ€æ±‚å’ŒæœŸæœ›",
                "å½“å‰å®¢æˆ·æ»¡æ„åº¦æ°´å¹³æœ‰å¾…æå‡ï¼Œå»ºè®®åˆ¶å®šç³»ç»Ÿæ€§çš„å®¢æˆ·ä½“éªŒæ”¹è¿›è®¡åˆ’",
                "åº”è¯¥é‡ç‚¹å…³æ³¨è¥é”€å†…å®¹çš„çœŸå®æ€§å’Œæ¸ é“çš„ä¾¿åˆ©æ€§ï¼ŒåŒæ—¶ä¿æŒäº§å“å“è´¨ä¼˜åŠ¿",
                "å»ºè®®å»ºç«‹æŒç»­çš„å®¢æˆ·åé¦ˆç›‘æ§æœºåˆ¶ï¼ŒåŠæ—¶è¯†åˆ«å’Œè§£å†³æ–°å‡ºç°çš„é—®é¢˜",
                "é€šè¿‡ä¼˜åŒ–å®¢æˆ·ä½“éªŒæ—…ç¨‹çš„å…³é”®è§¦ç‚¹ï¼Œæœ‰æœ›æ˜¾è‘—æå‡NPSåˆ†æ•°å’Œå®¢æˆ·å¿ è¯šåº¦"
            ],
            "business_questions": [
                "å¦‚ä½•åˆ¶å®šç»¼åˆæ€§çš„NPSæå‡æˆ˜ç•¥ï¼Ÿ",
                "å„éƒ¨é—¨åº”è¯¥å¦‚ä½•åä½œæ”¹å–„å®¢æˆ·ä½“éªŒï¼Ÿ",
                "å¦‚ä½•å¹³è¡¡çŸ­æœŸæ”¹è¿›å’Œé•¿æœŸå“ç‰Œå»ºè®¾ï¼Ÿ",
                "éœ€è¦è®¾ç«‹ä»€ä¹ˆKPIæ¥è·Ÿè¸ªæ”¹è¿›æ•ˆæœï¼Ÿ"
            ],
            "additional_info": self.additional_info,
            "summary_text": f"åŸºäºä¸ƒæ™ºèƒ½ä½“çš„å…¨æ–¹ä½NPSåˆ†ææ˜¾ç¤ºï¼Œä¼Šåˆ©é›†å›¢å½“å‰å®¢æˆ·æ»¡æ„åº¦å‘ˆç°å¤æ‚æ€åŠ¿ã€‚é€šè¿‡{len(all_agent_results)}ä¸ªç»´åº¦çš„æ·±å…¥å‰–æï¼Œå‘ç°æ¨èè€…ç¾¤ä½“è®¤å¯äº§å“å“è´¨å’Œ{', '.join(self.knowledge_base.get('key_brands', ['å®‰æ…•å¸Œ', 'é‡‘å…¸'])[:2])}ç­‰å“ç‰Œå¤šæ ·æ€§ï¼Œä½†éæ¨èè€…ä¸»è¦å…³æ³¨è¥é”€å®£ä¼ çœŸå®æ€§å’Œæ¸ é“ä¾¿åˆ©æ€§é—®é¢˜ã€‚å®¢æˆ·åé¦ˆå‘ˆç°ä¸¤æåˆ†åŒ–è¶‹åŠ¿ï¼Œéœ€è¦åˆ¶å®šç³»ç»Ÿæ€§çš„å®¢æˆ·ä½“éªŒæ”¹è¿›è®¡åˆ’ï¼Œé‡ç‚¹ä¼˜åŒ–è¥é”€å†…å®¹å’Œè´­ä¹°ä¾¿åˆ©æ€§ï¼ŒåŒæ—¶ä¿æŒäº§å“å“è´¨ä¼˜åŠ¿ï¼Œå»ºç«‹æŒç»­çš„å®¢æˆ·åé¦ˆç›‘æ§æœºåˆ¶ã€‚",
            "insight_summary": [
                f"1. å…¨é¢åˆ†æï¼šåŸºäº{len(all_agent_results)}ä¸ªç»´åº¦çš„æ·±åº¦æ´å¯Ÿ",
                "2. å“è´¨è®¤å¯ï¼šæ¨èè€…è®¤å¯äº§å“å“è´¨å’Œå¤šæ ·æ€§",
                "3. å…³é”®é—®é¢˜ï¼šè¥é”€å®£ä¼ å’Œæ¸ é“ä¾¿åˆ©æ€§éœ€æ”¹è¿›",
                "4. å®¢æˆ·åˆ†åŒ–ï¼šåé¦ˆå‘ˆç°ä¸¤æåˆ†åŒ–è¶‹åŠ¿",
                "5. æ”¹è¿›æ–¹å‘ï¼šç³»ç»Ÿæ€§å®¢æˆ·ä½“éªŒæå‡è®¡åˆ’",
                "6. è¥é”€ä¼˜åŒ–ï¼šé‡ç‚¹å…³æ³¨å†…å®¹çœŸå®æ€§",
                "7. æ¸ é“å»ºè®¾ï¼šæå‡è´­ä¹°ä¾¿åˆ©æ€§ä½“éªŒ",
                "8. ç›‘æ§æœºåˆ¶ï¼šå»ºç«‹æŒç»­åé¦ˆè·Ÿè¸ªç³»ç»Ÿ"
            ]
        }
        
        return analysis
    
    def _create_final_output(self, input_data, processed_data, agent_results, positive_set, negative_set):
        """Create final output structure"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        
        return {
            "timestamp": timestamp,
            "analysis_type": "ä¸ƒæ™ºèƒ½ä½“NPSåˆ†æç³»ç»Ÿ",
            "original_input": {
                "data_source": "ä¼Šåˆ©NPSè°ƒç ”é—®å·",
                "processed_responses": processed_data.total_responses,
                "valid_responses": processed_data.valid_responses,
                "survey_metadata": processed_data.survey_metadata
            },
            "agent_analysis_results": agent_results,
            "supporting_data": {
                "positive_set": positive_set,
                "negative_set": negative_set,
                "question_texts": self.question_texts
            },
            "summary_statistics": {
                "total_agents": len(agent_results),
                "total_statements": sum(len(result.get("statements", [])) for result in agent_results),
                "total_business_questions": sum(len(result.get("business_questions", [])) for result in agent_results),
                "total_ai_insights": sum(len(result.get("insight_summary", [])) for result in agent_results),
                "ai_powered_agents": sum(1 for result in agent_results if "ai_analysis" in result),
                "fallback_agents": sum(1 for result in agent_results if "ai_analysis" not in result),
                "analysis_coverage": "NPSå‡€å€¼ã€åˆ†å¸ƒã€æ­£é¢å› ç´ ã€è´Ÿé¢å› ç´ ã€å¼€æ”¾å›ç­”ã€ç»¼åˆæ´å¯Ÿ"
            }
        }