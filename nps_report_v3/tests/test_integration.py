"""
Integration tests for NPS V3 Analysis System.

Tests the complete workflow from raw data input through multi-agent processing
to final report generation, validating end-to-end system functionality.
"""

import pytest
import asyncio
import json
import tempfile
from pathlib import Path
from datetime import datetime
from unittest.mock import Mock, patch, AsyncMock
from typing import Dict, Any, List

# Import system components
from nps_report_v3.workflow.orchestrator import WorkflowOrchestrator
from nps_report_v3.agents.factory import AgentFactory
from nps_report_v3.generators.dual_output_generator import DualOutputGenerator
from nps_report_v3.models.response import NPSAnalysisResponse
from nps_report_v3.state import NPSAnalysisState, create_initial_state


class TestDataFactory:
    """Factory for creating test data for integration tests."""

    @staticmethod
    def create_sample_survey_data() -> List[Dict[str, Any]]:
        """Create sample survey response data."""
        return [
            {
                "response_id": "resp_001",
                "score": 9,
                "comment": "ä¼Šåˆ©å®‰æ…•å¸Œçš„å£æ„Ÿéå¸¸å¥½ï¼Œè´¨é‡ç¨³å®šï¼Œæ˜¯æˆ‘æœ€å–œæ¬¢çš„é…¸å¥¶å“ç‰Œã€‚åŒ…è£…ä¹Ÿå¾ˆç²¾ç¾ï¼Œå€¼å¾—æ¨èç»™æœ‹å‹ã€‚",
                "product": "å®‰æ…•å¸Œ",
                "region": "åä¸œåœ°åŒº",
                "channel": "çº¿ä¸Šå•†åŸ",
                "demographics": {"age_group": "25-35", "gender": "å¥³"}
            },
            {
                "response_id": "resp_002",
                "score": 8,
                "comment": "é‡‘å…¸ç‰›å¥¶çš„å“è´¨ä¸é”™ï¼Œä½†æ˜¯ä»·æ ¼æœ‰ç‚¹è´µã€‚å¸Œæœ›èƒ½æœ‰æ›´å¤šçš„ä¼˜æƒ æ´»åŠ¨ã€‚",
                "product": "é‡‘å…¸",
                "region": "ååŒ—åœ°åŒº",
                "channel": "è¿é”è¶…å¸‚",
                "demographics": {"age_group": "35-45", "gender": "ç”·"}
            },
            {
                "response_id": "resp_003",
                "score": 7,
                "comment": "èˆ’åŒ–å¥¶çš„å£æ„Ÿè¿˜å¯ä»¥ï¼Œä½†æ˜¯åŒ…è£…è®¾è®¡å¯ä»¥æ›´æ—¶å°šä¸€äº›ã€‚æœåŠ¡æ–¹é¢ä¹Ÿæœ‰æ”¹è¿›ç©ºé—´ã€‚",
                "product": "èˆ’åŒ–",
                "region": "åå—åœ°åŒº",
                "channel": "ä¾¿åˆ©åº—",
                "demographics": {"age_group": "18-25", "gender": "å¥³"}
            },
            {
                "response_id": "resp_004",
                "score": 10,
                "comment": "ä¼˜é…¸ä¹³çš„å‘³é“å¾ˆæ£’ï¼Œå­©å­ä»¬éƒ½å¾ˆå–œæ¬¢ã€‚è´¨é‡å¯é ï¼Œä¼šç»§ç»­è´­ä¹°ã€‚",
                "product": "ä¼˜é…¸ä¹³",
                "region": "è¥¿å—åœ°åŒº",
                "channel": "ä¸“å–åº—",
                "demographics": {"age_group": "35-45", "gender": "å¥³"}
            },
            {
                "response_id": "resp_005",
                "score": 6,
                "comment": "æœ€è¿‘ä¹°çš„å‘³å¯æ»‹å£æ„Ÿä¸å¦‚ä»¥å‰ï¼Œå¯èƒ½æ˜¯é…æ–¹æ”¹äº†ã€‚å¸Œæœ›èƒ½æ¢å¤åŸæ¥çš„å‘³é“ã€‚",
                "product": "å‘³å¯æ»‹",
                "region": "ä¸œåŒ—åœ°åŒº",
                "channel": "çº¿ä¸Šå•†åŸ",
                "demographics": {"age_group": "25-35", "gender": "ç”·"}
            },
            {
                "response_id": "resp_006",
                "score": 4,
                "comment": "å®¢æœæ€åº¦ä¸å¥½ï¼Œé…é€ä¹Ÿå¾ˆæ…¢ã€‚äº§å“è´¨é‡ä¹Ÿä¸ç¨³å®šï¼Œæœ‰æ—¶å€™å£æ„Ÿå¾ˆå·®ã€‚",
                "product": "å®‰æ…•å¸Œ",
                "region": "åä¸œåœ°åŒº",
                "channel": "çº¿ä¸Šå•†åŸ",
                "demographics": {"age_group": "45-55", "gender": "ç”·"}
            },
            {
                "response_id": "resp_007",
                "score": 3,
                "comment": "é‡‘å…¸ç‰›å¥¶æœ‰å¼‚å‘³ï¼Œå¯èƒ½æ˜¯ä¿å­˜ä¸å½“ã€‚é€€è´§æµç¨‹ä¹Ÿå¾ˆå¤æ‚ï¼Œä½“éªŒå¾ˆå·®ã€‚",
                "product": "é‡‘å…¸",
                "region": "ååŒ—åœ°åŒº",
                "channel": "è¿é”è¶…å¸‚",
                "demographics": {"age_group": "55+", "gender": "å¥³"}
            },
            {
                "response_id": "resp_008",
                "score": 8,
                "comment": "æ•´ä½“è¿˜ä¸é”™ï¼Œä½†å¸Œæœ›èƒ½æ¨å‡ºæ›´å¤šæ–°å£å‘³ã€‚ä»·æ ¼åˆç†ï¼Œè´¨é‡ç¨³å®šã€‚",
                "product": "èˆ’åŒ–",
                "region": "åå—åœ°åŒº",
                "channel": "ä¾¿åˆ©åº—",
                "demographics": {"age_group": "18-25", "gender": "ç”·"}
            },
            {
                "response_id": "resp_009",
                "score": 9,
                "comment": "QQæ˜Ÿå„¿ç«¥å¥¶å¾ˆå—å­©å­æ¬¢è¿ï¼Œè¥å…»ä¸°å¯Œï¼ŒåŒ…è£…å¯çˆ±ã€‚ä¼šæ¨èç»™å…¶ä»–å®¶é•¿ã€‚",
                "product": "QQæ˜Ÿ",
                "region": "è¥¿å—åœ°åŒº",
                "channel": "ä¸“å–åº—",
                "demographics": {"age_group": "35-45", "gender": "å¥³"}
            },
            {
                "response_id": "resp_010",
                "score": 5,
                "comment": "ä¼Šå°æ¬¢çš„å£æ„Ÿä¸€èˆ¬ï¼Œæ€§ä»·æ¯”ä¸é«˜ã€‚å¸Œæœ›èƒ½æ”¹è¿›äº§å“é…æ–¹ã€‚",
                "product": "ä¼Šå°æ¬¢",
                "region": "ä¸œåŒ—åœ°åŒº",
                "channel": "çº¿ä¸Šå•†åŸ",
                "demographics": {"age_group": "25-35", "gender": "å¥³"}
            }
        ]

    @staticmethod
    def create_workflow_config() -> Dict[str, Any]:
        """Create workflow configuration."""
        return {
            "enable_checkpointing": False,  # Disable for testing
            "enable_caching": False,
            "enable_profiling": True,
            "language": "zh-CN",
            "company_name": "ä¼Šåˆ©é›†å›¢",
            "analysis_depth": "comprehensive"
        }

    @staticmethod
    def create_mock_llm_responses() -> Dict[str, str]:
        """Create mock LLM responses for different agents."""
        return {
            "A0": json.dumps({
                "cleaned_responses": [
                    {"id": "resp_001", "score": 9, "comment": "æ­£é¢åé¦ˆï¼Œè´¨é‡å¥½"},
                    {"id": "resp_002", "score": 8, "comment": "æ­£é¢ä½†ä»·æ ¼å…³æ³¨"},
                    {"id": "resp_003", "score": 7, "comment": "ä¸­æ€§åé¦ˆ"},
                    {"id": "resp_004", "score": 10, "comment": "éå¸¸æ»¡æ„"},
                    {"id": "resp_005", "score": 6, "comment": "äº§å“è´¨é‡ä¸‹é™"},
                    {"id": "resp_006", "score": 4, "comment": "æœåŠ¡å’Œè´¨é‡é—®é¢˜"},
                    {"id": "resp_007", "score": 3, "comment": "äº§å“è´¨é‡ä¸¥é‡é—®é¢˜"},
                    {"id": "resp_008", "score": 8, "comment": "æ•´ä½“æ»¡æ„"},
                    {"id": "resp_009", "score": 9, "comment": "å„¿ç«¥äº§å“å—æ¬¢è¿"},
                    {"id": "resp_010", "score": 5, "comment": "æ€§ä»·æ¯”ä¸é«˜"}
                ],
                "data_quality": "good",
                "pii_removed": True
            }),
            "A1": json.dumps({
                "nps_score": 40,
                "promoter_count": 3,
                "passive_count": 3,
                "detractor_count": 4,
                "sample_size": 10,
                "statistical_significance": False
            }),
            "A2": json.dumps({
                "tagged_responses": [
                    {"id": "resp_001", "tags": ["è´¨é‡", "æ­£é¢", "æ¨è"]},
                    {"id": "resp_002", "tags": ["ä»·æ ¼", "ä¼˜æƒ ", "å»ºè®®"]},
                    {"id": "resp_003", "tags": ["åŒ…è£…", "æœåŠ¡", "æ”¹è¿›"]},
                    {"id": "resp_006", "tags": ["å®¢æœ", "é…é€", "è´¨é‡"]},
                    {"id": "resp_007", "tags": ["å¼‚å‘³", "é€€è´§", "ä½“éªŒ"]}
                ]
            }),
            "A3": json.dumps({
                "semantic_clusters": [
                    {
                        "cluster_id": 1,
                        "theme": "äº§å“è´¨é‡",
                        "keywords": ["è´¨é‡", "å£æ„Ÿ", "ç¨³å®š"],
                        "response_count": 4
                    },
                    {
                        "cluster_id": 2,
                        "theme": "ä»·æ ¼å…³æ³¨",
                        "keywords": ["ä»·æ ¼", "è´µ", "æ€§ä»·æ¯”"],
                        "response_count": 3
                    },
                    {
                        "cluster_id": 3,
                        "theme": "æœåŠ¡ä½“éªŒ",
                        "keywords": ["å®¢æœ", "é…é€", "æœåŠ¡"],
                        "response_count": 3
                    }
                ]
            }),
            "B1": json.dumps({
                "technical_requirements": [
                    "æ”¹è¿›äº§å“é…æ–¹ç¨³å®šæ€§",
                    "ä¼˜åŒ–åŒ…è£…è®¾è®¡",
                    "æå‡ä¿é²œæŠ€æœ¯"
                ]
            }),
            "B2": json.dumps({
                "passive_analysis": {
                    "conversion_opportunities": [
                        "ä»·æ ¼ä¼˜æƒ ç­–ç•¥",
                        "äº§å“åˆ›æ–°",
                        "æœåŠ¡æ”¹è¿›"
                    ],
                    "main_concerns": ["ä»·æ ¼", "æ–°åŠŸèƒ½éœ€æ±‚", "æœåŠ¡è´¨é‡"]
                }
            }),
            "B3": json.dumps({
                "detractor_analysis": {
                    "pain_points": [
                        {"issue": "äº§å“è´¨é‡ä¸ç¨³å®š", "severity": "high", "frequency": 3},
                        {"issue": "å®¢æœå“åº”æ…¢", "severity": "medium", "frequency": 2},
                        {"issue": "é€€è´§æµç¨‹å¤æ‚", "severity": "medium", "frequency": 1}
                    ],
                    "churn_risk": "high"
                }
            }),
            "C1": json.dumps({
                "strategic_recommendations": [
                    {
                        "title": "å»ºç«‹è´¨é‡ç®¡æ§ä½“ç³»",
                        "description": "åŠ å¼ºç”Ÿäº§è´¨é‡ç›‘æ§ï¼Œç¡®ä¿äº§å“ä¸€è‡´æ€§",
                        "priority": "high",
                        "timeline": "3-6ä¸ªæœˆ"
                    },
                    {
                        "title": "ä¼˜åŒ–å®¢æˆ·æœåŠ¡æµç¨‹",
                        "description": "æ”¹å–„å®¢æœå“åº”æ—¶é—´å’ŒæœåŠ¡è´¨é‡",
                        "priority": "medium",
                        "timeline": "1-3ä¸ªæœˆ"
                    }
                ]
            }),
            "C2": json.dumps({
                "product_recommendations": [
                    {
                        "category": "äº§å“æ”¹è¿›",
                        "recommendations": [
                            "ä¼˜åŒ–å®‰æ…•å¸Œé…æ–¹ï¼Œæå‡å£æ„Ÿç¨³å®šæ€§",
                            "å¼€å‘é‡‘å…¸æ–°å£å‘³ï¼Œæ»¡è¶³å¤šæ ·åŒ–éœ€æ±‚",
                            "æ”¹è¿›åŒ…è£…è®¾è®¡ï¼Œå¢å¼ºè§†è§‰å¸å¼•åŠ›"
                        ]
                    }
                ]
            }),
            "C5": json.dumps({
                "executive_synthesis": {
                    "key_findings": [
                        "æ•´ä½“NPSä¸º40ï¼Œå¤„äºè‰¯å¥½æ°´å¹³ä½†æœ‰æå‡ç©ºé—´",
                        "äº§å“è´¨é‡æ˜¯æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ï¼Œéœ€è¦æŒç»­ç»´æŠ¤",
                        "ä»·æ ¼æ•æ„Ÿåº¦è¾ƒé«˜ï¼Œéœ€è¦å¹³è¡¡ä»·å€¼ä¸æˆæœ¬",
                        "æœåŠ¡ä½“éªŒæ˜¯æ”¹è¿›é‡ç‚¹é¢†åŸŸ"
                    ],
                    "strategic_priorities": [
                        "è´¨é‡ç®¡æ§ä½“ç³»å»ºè®¾",
                        "å®¢æˆ·æœåŠ¡ä¼˜åŒ–",
                        "äº§å“åˆ›æ–°ä¸å·®å¼‚åŒ–",
                        "ä»·å€¼ä¼ æ’­ä¸å“ç‰Œå»ºè®¾"
                    ]
                }
            })
        }


class TestEndToEndWorkflow:
    """Test complete end-to-end workflow."""

    @pytest.fixture
    def sample_data(self):
        """Provide sample survey data."""
        return TestDataFactory.create_sample_survey_data()

    @pytest.fixture
    def workflow_config(self):
        """Provide workflow configuration."""
        return TestDataFactory.create_workflow_config()

    @pytest.fixture
    def mock_llm_responses(self):
        """Provide mock LLM responses."""
        return TestDataFactory.create_mock_llm_responses()

    @pytest.mark.asyncio
    async def test_complete_analysis_workflow(self, sample_data, workflow_config, mock_llm_responses):
        """Test complete analysis workflow from data input to final report."""

        with tempfile.TemporaryDirectory() as temp_dir:
            # Initialize orchestrator
            orchestrator = WorkflowOrchestrator(
                workflow_id="integration_test_001",
                enable_checkpointing=False,
                enable_caching=False
            )

            # Mock LLM calls for all agents
            async def mock_llm_call(prompt, agent_id=None, **kwargs):
                """Mock LLM call that returns appropriate response based on agent."""
                if agent_id and agent_id in mock_llm_responses:
                    return mock_llm_responses[agent_id]
                return json.dumps({"result": "mock_response", "agent_id": agent_id})

            # Patch LLM clients
            with patch('nps_report_v3.llm.llm_client.LLMClient.call_llm', new_callable=AsyncMock) as mock_call:
                mock_call.side_effect = mock_llm_call

                # Execute workflow
                try:
                    final_state = await orchestrator.execute(
                        raw_data=sample_data,
                        config=workflow_config
                    )

                    # Validate workflow completion
                    assert final_state["workflow_phase"] == "completed"
                    assert final_state["workflow_id"] == "integration_test_001"
                    assert "completion_time" in final_state

                    # Validate foundation pass results
                    assert "cleaned_data" in final_state
                    assert "nps_metrics" in final_state
                    assert "tagged_responses" in final_state
                    assert "semantic_clusters" in final_state

                    # Validate analysis pass results
                    assert "technical_requirements" in final_state or "analysis_insights" in final_state

                    # Validate consulting pass results
                    assert "strategic_recommendations" in final_state or "consulting_recommendations" in final_state

                    # Generate reports from final state
                    report_generator = DualOutputGenerator(
                        output_directory=temp_dir,
                        company_name="ä¼Šåˆ©é›†å›¢æµ‹è¯•"
                    )

                    report_package = await report_generator.generate_complete_report_package(final_state)

                    # Validate report generation
                    assert "report_id" in report_package
                    assert "generated_outputs" in report_package
                    assert report_package["generated_outputs"]["total_files"] > 0

                    return final_state, report_package

                except Exception as e:
                    # Log detailed error information for debugging
                    print(f"Integration test failed: {e}")
                    print(f"Exception type: {type(e)}")
                    import traceback
                    traceback.print_exc()
                    raise

    @pytest.mark.asyncio
    async def test_foundation_pass_only(self, sample_data, workflow_config):
        """Test foundation pass execution in isolation."""

        orchestrator = WorkflowOrchestrator(
            workflow_id="foundation_test",
            enable_checkpointing=False
        )

        # Create initial state
        state = create_initial_state(
            workflow_id="foundation_test",
            raw_data=sample_data,
            **workflow_config
        )

        # Add input_data structure
        state["input_data"] = {
            "survey_responses": sample_data,
            "responses": sample_data
        }

        # Mock foundation agents
        mock_responses = TestDataFactory.create_mock_llm_responses()

        async def mock_agent_execute(self, state):
            """Mock agent execution."""
            agent_id = self.agent_id
            mock_result = Mock()
            mock_result.status.value = "completed"

            if agent_id in mock_responses:
                mock_result.data = json.loads(mock_responses[agent_id])
            else:
                mock_result.data = {"result": f"mock_data_for_{agent_id}"}

            return mock_result

        # Patch agent execution
        with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = mock_agent_execute

            # Execute foundation pass
            result_state = await orchestrator._execute_foundation_pass(state)

            # Validate foundation pass results
            assert result_state["workflow_phase"] == "foundation"
            assert "cleaned_data" in result_state or "input_data" in result_state

    @pytest.mark.asyncio
    async def test_analysis_pass_parallel_execution(self, sample_data, workflow_config):
        """Test analysis pass with parallel agent execution."""

        orchestrator = WorkflowOrchestrator(
            workflow_id="analysis_test",
            enable_checkpointing=False
        )

        # Create state with foundation results
        state = create_initial_state(
            workflow_id="analysis_test",
            raw_data=sample_data,
            **workflow_config
        )

        # Add foundation results
        state.update({
            "input_data": {"survey_responses": sample_data},
            "workflow_phase": "analysis",
            "cleaned_data": {"quality": "good"},
            "nps_metrics": {"nps_score": 40, "sample_size": 10},
            "tagged_responses": [],
            "semantic_clusters": []
        })

        # Mock parallel agent execution
        async def mock_parallel_execution(self, state):
            """Mock parallel agent execution."""
            import asyncio
            await asyncio.sleep(0.1)  # Simulate processing time

            mock_result = Mock()
            mock_result.status.value = "completed"
            mock_result.data = {"agent_result": f"data_from_{self.agent_id}"}
            return mock_result

        with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = mock_parallel_execution

            # Execute analysis pass
            result_state = await orchestrator._execute_analysis_pass(state)

            # Validate parallel execution completed
            assert result_state["workflow_phase"] == "analysis"

    @pytest.mark.asyncio
    async def test_consulting_pass_confidence_constraints(self, sample_data, workflow_config):
        """Test consulting pass with confidence-based constraints."""

        orchestrator = WorkflowOrchestrator(
            workflow_id="consulting_test",
            enable_checkpointing=False
        )

        # Create state with analysis results
        state = create_initial_state(
            workflow_id="consulting_test",
            raw_data=sample_data,
            **workflow_config
        )

        # Add analysis results with different confidence levels
        state.update({
            "input_data": {"survey_responses": sample_data},
            "workflow_phase": "consulting",
            "nps_metrics": {
                "nps_score": 40,
                "sample_size": 10,
                "statistical_significance": False  # Low confidence scenario
            },
            "cleaned_data": {"data_quality": "medium"},
            "technical_requirements": [],
            "semantic_clusters": [],
            "tagged_responses": []
        })

        # Mock consulting agents with confidence checks
        async def mock_consulting_execution(self, state):
            """Mock consulting agent execution with confidence awareness."""
            mock_result = Mock()
            mock_result.status.value = "completed"
            mock_result.data = {
                "recommendations": [f"Low confidence recommendation from {self.agent_id}"],
                "confidence_applied": True
            }
            return mock_result

        with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = mock_consulting_execution

            # Execute consulting pass
            result_state = await orchestrator._execute_consulting_pass(state)

            # Validate consulting pass handled confidence constraints
            assert result_state["workflow_phase"] == "consulting"

    @pytest.mark.asyncio
    async def test_error_recovery_and_resilience(self, sample_data, workflow_config):
        """Test system resilience and error recovery."""

        orchestrator = WorkflowOrchestrator(
            workflow_id="resilience_test",
            enable_checkpointing=False
        )

        # Simulate agent failure
        async def mock_failing_agent(self, state):
            """Mock agent that sometimes fails."""
            if self.agent_id == "B2":  # Simulate B2 agent failure
                raise RuntimeError(f"Simulated failure in agent {self.agent_id}")

            mock_result = Mock()
            mock_result.status.value = "completed"
            mock_result.data = {"result": f"success_from_{self.agent_id}"}
            return mock_result

        with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = mock_failing_agent

            # Execute workflow - should handle agent failure gracefully
            with pytest.raises(RuntimeError):  # Expecting failure propagation
                await orchestrator.execute(sample_data, workflow_config)

    @pytest.mark.asyncio
    async def test_large_dataset_performance(self):
        """Test performance with large dataset."""

        # Create large dataset
        large_data = []
        for i in range(1000):  # 1000 responses
            large_data.append({
                "response_id": f"large_resp_{i:04d}",
                "score": (i % 11),  # Scores from 0-10
                "comment": f"è¿™æ˜¯ç¬¬{i+1}ä¸ªæµ‹è¯•è¯„è®ºï¼ŒåŒ…å«å®¢æˆ·å¯¹äº§å“çš„è¯¦ç»†åé¦ˆå’Œå»ºè®®ã€‚äº§å“è´¨é‡ã€æœåŠ¡ä½“éªŒã€ä»·æ ¼å› ç´ éƒ½æ˜¯å®¢æˆ·å…³æ³¨çš„é‡ç‚¹ã€‚",
                "product": ["å®‰æ…•å¸Œ", "é‡‘å…¸", "èˆ’åŒ–", "ä¼˜é…¸ä¹³", "å‘³å¯æ»‹"][i % 5],
                "region": ["ååŒ—", "åä¸œ", "åå—", "è¥¿å—", "ä¸œåŒ—"][i % 5],
                "channel": ["çº¿ä¸Š", "è¶…å¸‚", "ä¾¿åˆ©åº—", "ä¸“å–åº—"][i % 4]
            })

        # Test with performance monitoring
        import time
        start_time = time.time()

        with tempfile.TemporaryDirectory() as temp_dir:
            # Mock fast agent responses for performance test
            async def mock_fast_agent(self, state):
                """Mock agent with minimal processing time."""
                mock_result = Mock()
                mock_result.status.value = "completed"
                mock_result.data = {
                    "processed_count": len(state.get("input_data", {}).get("survey_responses", [])),
                    "agent_id": self.agent_id
                }
                return mock_result

            orchestrator = WorkflowOrchestrator(
                workflow_id="performance_test",
                enable_checkpointing=False
            )

            with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
                mock_execute.side_effect = mock_fast_agent

                try:
                    # Execute workflow
                    result_state = await orchestrator.execute(large_data, {"enable_profiling": True})

                    processing_time = time.time() - start_time

                    # Performance assertions
                    assert processing_time < 60  # Should complete within 1 minute
                    assert result_state["workflow_phase"] == "completed"

                    # Generate reports
                    report_generator = DualOutputGenerator(output_directory=temp_dir)
                    report_package = await report_generator.generate_complete_report_package(result_state)

                    total_time = time.time() - start_time
                    assert total_time < 120  # Total time including reports < 2 minutes

                    print(f"Performance test completed in {total_time:.2f} seconds for 1000 responses")

                except Exception as e:
                    print(f"Performance test failed: {e}")
                    raise

    @pytest.mark.asyncio
    async def test_multilingual_content_handling(self):
        """Test handling of multilingual content."""

        multilingual_data = [
            {
                "response_id": "ml_001",
                "score": 9,
                "comment": "ä¼Šåˆ©äº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼Excellence quality and taste.",
                "product": "å®‰æ…•å¸Œ",
                "language": "zh-en"
            },
            {
                "response_id": "ml_002",
                "score": 7,
                "comment": "Good product, but expensive. äº§å“ä¸é”™ä½†ä»·æ ¼æœ‰ç‚¹è´µã€‚",
                "product": "é‡‘å…¸",
                "language": "en-zh"
            },
            {
                "response_id": "ml_003",
                "score": 8,
                "comment": "å¾ˆæ£’çš„äº§å“ï¼ğŸ‘ğŸ» Really love the taste and quality! äº”æ˜Ÿæ¨èâ­â­â­â­â­",
                "product": "ä¼˜é…¸ä¹³",
                "language": "zh-en-emoji"
            }
        ]

        orchestrator = WorkflowOrchestrator(
            workflow_id="multilingual_test",
            enable_checkpointing=False
        )

        # Mock agents that handle multilingual content
        async def mock_multilingual_agent(self, state):
            """Mock agent that processes multilingual content."""
            mock_result = Mock()
            mock_result.status.value = "completed"
            mock_result.data = {
                "multilingual_processed": True,
                "languages_detected": ["zh", "en"],
                "emoji_support": True
            }
            return mock_result

        with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = mock_multilingual_agent

            # Execute with multilingual data
            result_state = await orchestrator.execute(multilingual_data, {"language": "auto"})

            # Validate multilingual handling
            assert result_state["workflow_phase"] == "completed"


class TestReportIntegration:
    """Test report generation integration with workflow results."""

    @pytest.mark.asyncio
    async def test_workflow_to_report_integration(self):
        """Test seamless integration from workflow to report generation."""

        # Create mock workflow result
        workflow_result = {
            "workflow_id": "integration_report_test",
            "workflow_phase": "completed",
            "completion_time": datetime.now().isoformat(),
            "input_data": {"survey_responses": TestDataFactory.create_sample_survey_data()},
            "nps_metrics": {
                "nps_score": 45,
                "promoter_count": 4,
                "passive_count": 3,
                "detractor_count": 3,
                "sample_size": 10,
                "statistical_significance": False
            },
            "cleaned_data": {"data_quality": "good"},
            "tagged_responses": [{"id": "001", "tags": ["è´¨é‡", "å¥½"]}],
            "semantic_clusters": [{"theme": "äº§å“è´¨é‡", "count": 5}],
            "technical_requirements": ["æ”¹è¿›é…æ–¹"],
            "passive_analysis": {"conversion_rate": "medium"},
            "detractor_analysis": {"churn_risk": "low"},
            "strategic_recommendations": [{"title": "è´¨é‡æå‡", "priority": "high"}]
        }

        with tempfile.TemporaryDirectory() as temp_dir:
            # Generate reports from workflow result
            report_generator = DualOutputGenerator(
                output_directory=temp_dir,
                company_name="ä¼Šåˆ©é›†å›¢é›†æˆæµ‹è¯•"
            )

            report_package = await report_generator.generate_complete_report_package(
                workflow_result,
                report_options={
                    "generate_json": True,
                    "generate_html": True,
                    "generate_summary": True
                }
            )

            # Validate integration
            assert report_package["package_info"]["company_name"] == "ä¼Šåˆ©é›†å›¢é›†æˆæµ‹è¯•"
            assert report_package["analysis_summary"]["nps_score"] == 45
            assert report_package["generated_outputs"]["total_files"] >= 2

            # Validate file existence and content
            json_files = report_package["generated_outputs"]["json_files"]
            html_files = report_package["generated_outputs"]["html_files"]

            for file_path in json_files.values():
                assert Path(file_path).exists()
                assert Path(file_path).stat().st_size > 0

            for file_path in html_files.values():
                assert Path(file_path).exists()
                assert Path(file_path).stat().st_size > 0

                # Validate HTML content
                content = Path(file_path).read_text(encoding='utf-8')
                assert "ä¼Šåˆ©é›†å›¢é›†æˆæµ‹è¯•" in content
                assert "45" in content  # NPS score should be in HTML

    @pytest.mark.asyncio
    async def test_report_quality_validation_integration(self):
        """Test integrated report quality validation."""

        from nps_report_v3.generators.html_report_generator import validate_report_quality

        # Create and generate a report
        with tempfile.TemporaryDirectory() as temp_dir:
            report_generator = DualOutputGenerator(output_directory=temp_dir)

            # Mock workflow result
            mock_result = {
                "workflow_id": "quality_test",
                "nps_metrics": {"nps_score": 50, "sample_size": 100},
                "executive_dashboard": {
                    "executive_summary": "æµ‹è¯•æ‘˜è¦",
                    "top_recommendations": [],
                    "risk_alerts": [],
                    "key_performance_indicators": {}
                }
            }

            # Generate HTML report
            html_path = await report_generator.generate_html_only(mock_result, "executive")

            # Validate report quality
            html_content = html_path.read_text(encoding='utf-8')
            quality_results = validate_report_quality(html_content)

            # Assert quality standards
            assert quality_results["overall_score"] >= 60  # Minimum acceptable quality
            assert quality_results["checks"]["html_structure"]

            if quality_results["overall_score"] < 80:
                print("Quality recommendations:", quality_results["recommendations"])


class TestDataPipeline:
    """Test data flow through the complete pipeline."""

    @pytest.mark.asyncio
    async def test_data_consistency_through_pipeline(self):
        """Test data consistency from input to final output."""

        # Track specific data point through pipeline
        tracked_response = {
            "response_id": "tracked_001",
            "score": 9,
            "comment": "ä¼Šåˆ©å®‰æ…•å¸Œé…¸å¥¶çš„å“è´¨éå¸¸æ£’ï¼Œå£æ„Ÿä¸æ»‘ï¼Œè¥å…»ä¸°å¯Œã€‚åŒ…è£…è®¾è®¡ä¹Ÿå¾ˆç²¾ç¾ï¼Œæ˜¯æˆ‘å®¶å†°ç®±é‡Œçš„å¿…å¤‡å“ã€‚å¼ºçƒˆæ¨èç»™æ‰€æœ‰æœ‹å‹ï¼",
            "product": "å®‰æ…•å¸Œ",
            "region": "åä¸œåœ°åŒº",
            "channel": "çº¿ä¸Šå•†åŸ"
        }

        input_data = [tracked_response] + TestDataFactory.create_sample_survey_data()

        # Mock agents to preserve tracked data
        def create_preserving_mock(agent_id):
            async def preserving_mock(self, state):
                mock_result = Mock()
                mock_result.status.value = "completed"

                # Preserve tracked response in results
                mock_result.data = {
                    "agent_id": agent_id,
                    "processed_tracked": True,
                    "tracked_response_found": any(
                        r.get("response_id") == "tracked_001"
                        for r in state.get("input_data", {}).get("survey_responses", [])
                    )
                }
                return mock_result
            return preserving_mock

        # Execute workflow with tracking
        orchestrator = WorkflowOrchestrator(
            workflow_id="data_consistency_test",
            enable_checkpointing=False
        )

        with patch('nps_report_v3.agents.base.BaseAgent.execute', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = create_preserving_mock("test_agent")

            result_state = await orchestrator.execute(input_data, {"track_data": True})

            # Verify tracked data presence in final state
            assert result_state["workflow_phase"] == "completed"

            # Check that original input is preserved
            original_responses = result_state.get("input_data", {}).get("survey_responses", [])
            tracked_found = any(r.get("response_id") == "tracked_001" for r in original_responses)
            assert tracked_found, "Tracked response lost during processing"

            # Generate report and verify data consistency
            with tempfile.TemporaryDirectory() as temp_dir:
                report_generator = DualOutputGenerator(output_directory=temp_dir)
                report_package = await report_generator.generate_complete_report_package(result_state)

                # Check JSON output contains original data
                json_files = report_package["generated_outputs"]["json_files"]
                main_json_path = json_files.get("main_results")

                if main_json_path:
                    with open(main_json_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)

                    # Verify data integrity in JSON output
                    assert "analysis_results" in json_data
                    # Could add more specific checks here


if __name__ == "__main__":
    # Run integration tests
    pytest.main([__file__, "-v", "-s", "--tb=short"])