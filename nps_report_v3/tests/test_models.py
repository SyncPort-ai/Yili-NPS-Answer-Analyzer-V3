"""
Comprehensive tests for NPS V3 Pydantic models.

Tests cover model validation, serialization, deserialization,
and edge cases for all response models.
"""

import pytest
from datetime import datetime
from typing import Dict, Any, List
from pydantic import ValidationError
import json

from nps_report_v3.models.response import (
    NPSMetrics,
    ConfidenceAssessment,
    AgentInsight,
    BusinessRecommendation,
    ExecutiveDashboard,
    NPSAnalysisResponse,
    build_analysis_response,
    create_minimal_response,
    validate_agent_insights
)


class TestNPSMetrics:
    """Test NPS metrics model."""

    def test_valid_nps_metrics(self):
        """Test creation of valid NPS metrics."""
        metrics = NPSMetrics(
            nps_score=45,
            promoter_count=50,
            passive_count=30,
            detractor_count=20,
            sample_size=100,
            statistical_significance=True
        )

        assert metrics.nps_score == 45
        assert metrics.promoter_count == 50
        assert metrics.passive_count == 30
        assert metrics.detractor_count == 20
        assert metrics.sample_size == 100
        assert metrics.statistical_significance is True

    def test_nps_score_validation(self):
        """Test NPS score validation."""
        # Valid NPS scores
        valid_scores = [-100, -50, 0, 25, 50, 75, 100]
        for score in valid_scores:
            metrics = NPSMetrics(
                nps_score=score,
                promoter_count=10,
                passive_count=10,
                detractor_count=10,
                sample_size=30
            )
            assert metrics.nps_score == score

        # Invalid NPS scores
        invalid_scores = [-101, 101, 150, -200]
        for score in invalid_scores:
            with pytest.raises(ValidationError):
                NPSMetrics(
                    nps_score=score,
                    promoter_count=10,
                    passive_count=10,
                    detractor_count=10,
                    sample_size=30
                )

    def test_count_validation(self):
        """Test count field validation."""
        # Negative counts should fail
        with pytest.raises(ValidationError):
            NPSMetrics(
                nps_score=0,
                promoter_count=-1,
                passive_count=10,
                detractor_count=10,
                sample_size=30
            )

    def test_sample_size_consistency(self):
        """Test sample size consistency validation."""
        # Sample size should be at least the sum of counts
        metrics = NPSMetrics(
            nps_score=0,
            promoter_count=10,
            passive_count=20,
            detractor_count=30,
            sample_size=60  # Equals sum
        )
        assert metrics.sample_size == 60

        # Sample size can be larger (some responses might not have scores)
        metrics = NPSMetrics(
            nps_score=0,
            promoter_count=10,
            passive_count=20,
            detractor_count=30,
            sample_size=80  # Larger than sum
        )
        assert metrics.sample_size == 80

    def test_nps_metrics_serialization(self):
        """Test NPS metrics JSON serialization."""
        metrics = NPSMetrics(
            nps_score=45,
            promoter_count=50,
            passive_count=30,
            detractor_count=20,
            sample_size=100,
            statistical_significance=True
        )

        # Test dict conversion
        metrics_dict = metrics.dict()
        assert metrics_dict["nps_score"] == 45
        assert metrics_dict["statistical_significance"] is True

        # Test JSON serialization
        json_str = metrics.json()
        parsed_data = json.loads(json_str)
        assert parsed_data["nps_score"] == 45

    def test_nps_classification_property(self):
        """Test NPS classification computed property."""
        # Excellent NPS
        excellent_metrics = NPSMetrics(
            nps_score=75, promoter_count=80, passive_count=10,
            detractor_count=5, sample_size=95
        )
        # Would need to implement classification property in model

        # Poor NPS
        poor_metrics = NPSMetrics(
            nps_score=-30, promoter_count=10, passive_count=20,
            detractor_count=70, sample_size=100
        )
        # Would need to implement classification property in model


class TestConfidenceAssessment:
    """Test confidence assessment model."""

    def test_valid_confidence_assessment(self):
        """Test creation of valid confidence assessment."""
        assessment = ConfidenceAssessment(
            overall_confidence_score=0.75,
            overall_confidence_text="‰∏≠",
            data_quality_score=0.80,
            analysis_completeness_score=0.70,
            statistical_significance_score=0.75
        )

        assert assessment.overall_confidence_score == 0.75
        assert assessment.overall_confidence_text == "‰∏≠"
        assert assessment.data_quality_score == 0.80

    def test_confidence_score_validation(self):
        """Test confidence score validation (0.0 to 1.0)."""
        # Valid scores
        valid_scores = [0.0, 0.5, 1.0, 0.75, 0.123]
        for score in valid_scores:
            assessment = ConfidenceAssessment(
                overall_confidence_score=score,
                overall_confidence_text="ÊµãËØï",
                data_quality_score=score,
                analysis_completeness_score=score,
                statistical_significance_score=score
            )
            assert assessment.overall_confidence_score == score

        # Invalid scores
        invalid_scores = [-0.1, 1.1, 2.0, -1.0]
        for score in invalid_scores:
            with pytest.raises(ValidationError):
                ConfidenceAssessment(
                    overall_confidence_score=score,
                    overall_confidence_text="ÊµãËØï",
                    data_quality_score=0.5,
                    analysis_completeness_score=0.5,
                    statistical_significance_score=0.5
                )

    def test_confidence_text_validation(self):
        """Test confidence text validation."""
        valid_texts = ["È´ò", "‰∏≠", "‰Ωé", "excellent", "good", "poor"]
        for text in valid_texts:
            assessment = ConfidenceAssessment(
                overall_confidence_score=0.5,
                overall_confidence_text=text,
                data_quality_score=0.5,
                analysis_completeness_score=0.5,
                statistical_significance_score=0.5
            )
            assert assessment.overall_confidence_text == text

        # Empty text should fail
        with pytest.raises(ValidationError):
            ConfidenceAssessment(
                overall_confidence_score=0.5,
                overall_confidence_text="",
                data_quality_score=0.5,
                analysis_completeness_score=0.5,
                statistical_significance_score=0.5
            )


class TestAgentInsight:
    """Test agent insight model."""

    def test_valid_agent_insight(self):
        """Test creation of valid agent insight."""
        insight = AgentInsight(
            agent_id="B1",
            title="ÊµãËØïÊ¥ûÂØü",
            summary="ÊµãËØïÊëòË¶Å",
            content="ËØ¶ÁªÜÁöÑÊ¥ûÂØüÂÜÖÂÆπÔºåÂåÖÂê´ÂàÜÊûêÁªìÊûúÂíåÂª∫ËÆÆ„ÄÇ",
            category="ÊäÄÊúØÂàÜÊûê",
            priority="È´ò",
            confidence=0.85,
            impact_score=0.75,
            timestamp=datetime.now().isoformat()
        )

        assert insight.agent_id == "B1"
        assert insight.title == "ÊµãËØïÊ¥ûÂØü"
        assert insight.priority == "È´ò"
        assert insight.confidence == 0.85

    def test_agent_id_validation(self):
        """Test agent ID validation."""
        valid_ids = ["A0", "A1", "B1", "B9", "C1", "C5"]
        for agent_id in valid_ids:
            insight = AgentInsight(
                agent_id=agent_id,
                title="ÊµãËØï",
                summary="ÊµãËØï",
                content="ÊµãËØïÂÜÖÂÆπ",
                category="ÊµãËØï",
                priority="‰∏≠",
                confidence=0.5,
                impact_score=0.5
            )
            assert insight.agent_id == agent_id

        # Invalid agent IDs
        invalid_ids = ["", "X1", "A10", "invalid"]
        for agent_id in invalid_ids:
            with pytest.raises(ValidationError):
                AgentInsight(
                    agent_id=agent_id,
                    title="ÊµãËØï",
                    summary="ÊµãËØï",
                    content="ÊµãËØïÂÜÖÂÆπ",
                    category="ÊµãËØï",
                    priority="‰∏≠",
                    confidence=0.5,
                    impact_score=0.5
                )

    def test_priority_validation(self):
        """Test priority validation."""
        valid_priorities = ["È´ò", "‰∏≠", "‰Ωé", "high", "medium", "low"]
        for priority in valid_priorities:
            insight = AgentInsight(
                agent_id="B1",
                title="ÊµãËØï",
                summary="ÊµãËØï",
                content="ÊµãËØïÂÜÖÂÆπ",
                category="ÊµãËØï",
                priority=priority,
                confidence=0.5,
                impact_score=0.5
            )
            assert insight.priority == priority

        # Invalid priorities
        invalid_priorities = ["", "urgent", "critical", "Êó†Êïà"]
        for priority in invalid_priorities:
            with pytest.raises(ValidationError):
                AgentInsight(
                    agent_id="B1",
                    title="ÊµãËØï",
                    summary="ÊµãËØï",
                    content="ÊµãËØïÂÜÖÂÆπ",
                    category="ÊµãËØï",
                    priority=priority,
                    confidence=0.5,
                    impact_score=0.5
                )

    def test_content_length_validation(self):
        """Test content length validation."""
        # Very short content should fail
        with pytest.raises(ValidationError):
            AgentInsight(
                agent_id="B1",
                title="ÊµãËØï",
                summary="ÊµãËØï",
                content="Áü≠",  # Too short
                category="ÊµãËØï",
                priority="‰∏≠",
                confidence=0.5,
                impact_score=0.5
            )

        # Normal content should pass
        insight = AgentInsight(
            agent_id="B1",
            title="ÊµãËØï",
            summary="ÊµãËØï",
            content="ËøôÊòØ‰∏Ä‰∏™Ë∂≥Â§üÈïøÁöÑÂÜÖÂÆπÔºåÂåÖÂê´‰∫ÜËØ¶ÁªÜÁöÑÂàÜÊûêÂíåÂª∫ËÆÆ„ÄÇ",
            category="ÊµãËØï",
            priority="‰∏≠",
            confidence=0.5,
            impact_score=0.5
        )
        assert len(insight.content) >= 10

    def test_timestamp_validation(self):
        """Test timestamp validation."""
        # Valid ISO timestamps
        valid_timestamps = [
            datetime.now().isoformat(),
            "2024-01-01T12:00:00",
            "2024-01-01T12:00:00.000Z"
        ]

        for timestamp in valid_timestamps:
            insight = AgentInsight(
                agent_id="B1",
                title="ÊµãËØï",
                summary="ÊµãËØï",
                content="ÊµãËØïÂÜÖÂÆπËØ¶ÁªÜ‰ø°ÊÅØ",
                category="ÊµãËØï",
                priority="‰∏≠",
                confidence=0.5,
                impact_score=0.5,
                timestamp=timestamp
            )
            assert insight.timestamp == timestamp


class TestBusinessRecommendation:
    """Test business recommendation model."""

    def test_valid_business_recommendation(self):
        """Test creation of valid business recommendation."""
        recommendation = BusinessRecommendation(
            title="‰ºòÂåñÂÆ¢Êà∑‰ΩìÈ™åÊµÅÁ®ã",
            description="ÈÄöËøáÊîπËøõÊúçÂä°ÊµÅÁ®ãÂíåÊäÄÊúØÊâãÊÆµÔºåÊèêÂçáÊï¥‰ΩìÂÆ¢Êà∑‰ΩìÈ™åË¥®Èáè„ÄÇ",
            category="ÂÆ¢Êà∑‰ΩìÈ™å",
            priority="È´ò",
            expected_impact="ÊòæËëóÊèêÂçáÂÆ¢Êà∑Êª°ÊÑèÂ∫¶ÂíåÂø†ËØöÂ∫¶",
            confidence_score=0.85,
            implementation_timeline="3-6‰∏™ÊúàÂÜÖÂÆåÊàê‰∏ªË¶ÅÊîπËøõÊé™ÊñΩ"
        )

        assert recommendation.title == "‰ºòÂåñÂÆ¢Êà∑‰ΩìÈ™åÊµÅÁ®ã"
        assert recommendation.priority == "È´ò"
        assert recommendation.confidence_score == 0.85

    def test_required_fields(self):
        """Test required fields validation."""
        # Missing title should fail
        with pytest.raises(ValidationError):
            BusinessRecommendation(
                description="ÊèèËø∞",
                category="Á±ªÂà´",
                priority="‰∏≠",
                confidence_score=0.5
            )

        # Missing description should fail
        with pytest.raises(ValidationError):
            BusinessRecommendation(
                title="Ê†áÈ¢ò",
                category="Á±ªÂà´",
                priority="‰∏≠",
                confidence_score=0.5
            )

    def test_optional_fields(self):
        """Test optional fields handling."""
        # Minimal recommendation
        recommendation = BusinessRecommendation(
            title="Âü∫Á°ÄÂª∫ËÆÆ",
            description="Âü∫Á°ÄÊèèËø∞ÂÜÖÂÆπ",
            priority="‰∏≠",
            confidence_score=0.5
        )

        assert recommendation.category is None
        assert recommendation.expected_impact is None
        assert recommendation.implementation_timeline is None

    def test_confidence_score_validation(self):
        """Test confidence score validation for recommendations."""
        # Valid confidence scores
        valid_scores = [0.0, 0.5, 1.0, 0.75]
        for score in valid_scores:
            recommendation = BusinessRecommendation(
                title="ÊµãËØïÂª∫ËÆÆ",
                description="ÊµãËØïÊèèËø∞",
                priority="‰∏≠",
                confidence_score=score
            )
            assert recommendation.confidence_score == score

        # Invalid confidence scores
        invalid_scores = [-0.1, 1.1, 2.0]
        for score in invalid_scores:
            with pytest.raises(ValidationError):
                BusinessRecommendation(
                    title="ÊµãËØïÂª∫ËÆÆ",
                    description="ÊµãËØïÊèèËø∞",
                    priority="‰∏≠",
                    confidence_score=score
                )


class TestExecutiveDashboard:
    """Test executive dashboard model."""

    def test_valid_executive_dashboard(self):
        """Test creation of valid executive dashboard."""
        recommendations = [
            BusinessRecommendation(
                title="Âª∫ËÆÆ1",
                description="Âª∫ËÆÆÊèèËø∞1",
                priority="È´ò",
                confidence_score=0.8
            ),
            BusinessRecommendation(
                title="Âª∫ËÆÆ2",
                description="Âª∫ËÆÆÊèèËø∞2",
                priority="‰∏≠",
                confidence_score=0.7
            )
        ]

        dashboard = ExecutiveDashboard(
            executive_summary="Êï¥‰ΩìÂàÜÊûêÊëòË¶ÅÔºåÂåÖÂê´ÂÖ≥ÈîÆÂèëÁé∞ÂíåÂª∫ËÆÆ„ÄÇ",
            top_recommendations=recommendations,
            risk_alerts=["È£éÈô©ÊèêÁ§∫1", "È£éÈô©ÊèêÁ§∫2"],
            key_performance_indicators={
                "ÂÆ¢Êà∑Êª°ÊÑèÂ∫¶": "75%",
                "NPSÂæóÂàÜ": "45",
                "ÂìçÂ∫îÊó∂Èó¥": "24Â∞èÊó∂"
            }
        )

        assert "Êï¥‰ΩìÂàÜÊûêÊëòË¶Å" in dashboard.executive_summary
        assert len(dashboard.top_recommendations) == 2
        assert len(dashboard.risk_alerts) == 2
        assert "ÂÆ¢Êà∑Êª°ÊÑèÂ∫¶" in dashboard.key_performance_indicators

    def test_empty_collections(self):
        """Test handling of empty collections."""
        dashboard = ExecutiveDashboard(
            executive_summary="Âü∫Á°ÄÊëòË¶Å",
            top_recommendations=[],
            risk_alerts=[],
            key_performance_indicators={}
        )

        assert dashboard.executive_summary == "Âü∫Á°ÄÊëòË¶Å"
        assert len(dashboard.top_recommendations) == 0
        assert len(dashboard.risk_alerts) == 0
        assert len(dashboard.key_performance_indicators) == 0

    def test_kpi_validation(self):
        """Test KPI dictionary validation."""
        # Valid KPIs
        valid_kpis = {
            "metric1": "value1",
            "ÊåáÊ†á2": "Êï∞ÂÄº2",
            "NPS": "45",
            "satisfaction_rate": "85%"
        }

        dashboard = ExecutiveDashboard(
            executive_summary="ÊµãËØïÊëòË¶Å",
            top_recommendations=[],
            risk_alerts=[],
            key_performance_indicators=valid_kpis
        )

        assert dashboard.key_performance_indicators == valid_kpis


class TestNPSAnalysisResponse:
    """Test complete NPS analysis response model."""

    def create_complete_response(self) -> NPSAnalysisResponse:
        """Create complete test response."""
        return NPSAnalysisResponse(
            response_id="test_response_123",
            nps_metrics=NPSMetrics(
                nps_score=45,
                promoter_count=50,
                passive_count=30,
                detractor_count=20,
                sample_size=100,
                statistical_significance=True
            ),
            confidence_assessment=ConfidenceAssessment(
                overall_confidence_score=0.75,
                overall_confidence_text="‰∏≠",
                data_quality_score=0.80,
                analysis_completeness_score=0.70,
                statistical_significance_score=0.75
            ),
            foundation_insights=[
                AgentInsight(
                    agent_id="A0",
                    title="Êï∞ÊçÆÊ∏ÖÊ¥óÁªìÊûú",
                    summary="Êï∞ÊçÆË¥®ÈáèËâØÂ•Ω",
                    content="ÂÆåÊàêÊï∞ÊçÆÊ∏ÖÊ¥óÂíåÈ¢ÑÂ§ÑÁêÜÔºåÊï∞ÊçÆË¥®ÈáèËØÑ‰º∞ÁªìÊûúËâØÂ•Ω„ÄÇ",
                    category="Êï∞ÊçÆÂ§ÑÁêÜ",
                    priority="‰Ωé",
                    confidence=0.95,
                    impact_score=0.85
                )
            ],
            analysis_insights=[
                AgentInsight(
                    agent_id="B1",
                    title="ÊäÄÊúØÈúÄÊ±ÇÂàÜÊûê",
                    summary="ÂÆ¢Êà∑ÊäÄÊúØÈúÄÊ±ÇÂàÜÊûê",
                    content="ÂàÜÊûêÂÆ¢Êà∑ÂØπ‰∫ßÂìÅÊäÄÊúØÂäüËÉΩÁöÑÈúÄÊ±ÇÂíåÊúüÊúõÔºåËØÜÂà´ÊîπËøõÊú∫‰ºö„ÄÇ",
                    category="ÊäÄÊúØÂàÜÊûê",
                    priority="È´ò",
                    confidence=0.82,
                    impact_score=0.78
                )
            ],
            consulting_recommendations=[
                BusinessRecommendation(
                    title="‰ºòÂåñ‰∫ßÂìÅÊäÄÊúØÊû∂ÊûÑ",
                    description="Âü∫‰∫éÂÆ¢Êà∑ÈúÄÊ±ÇÂàÜÊûêÔºå‰ºòÂåñ‰∫ßÂìÅÊäÄÊúØÊû∂ÊûÑÂíåÂäüËÉΩËÆæËÆ°„ÄÇ",
                    category="ÊäÄÊúØÊîπËøõ",
                    priority="È´ò",
                    expected_impact="ÊèêÂçáÁî®Êà∑‰ΩìÈ™å",
                    confidence_score=0.85,
                    implementation_timeline="3-6‰∏™Êúà"
                )
            ],
            executive_dashboard=ExecutiveDashboard(
                executive_summary="Êï¥‰ΩìÂÆ¢Êà∑Êª°ÊÑèÂ∫¶‰∏≠Á≠âÔºåÂ≠òÂú®ÊòéÊòæÊîπËøõÁ©∫Èó¥„ÄÇ",
                top_recommendations=[
                    BusinessRecommendation(
                        title="Âª∫Á´ãÂÆ¢Êà∑ÂèçÈ¶àÊú∫Âà∂",
                        description="Âª∫Á´ãÁ≥ªÁªüÊÄßÁöÑÂÆ¢Êà∑ÂèçÈ¶àÊî∂ÈõÜÂíåÂ§ÑÁêÜÊú∫Âà∂„ÄÇ",
                        priority="‰∏≠",
                        confidence_score=0.75
                    )
                ],
                risk_alerts=["ÂÆ¢Êà∑ÊµÅÂ§±È£éÈô©Â¢ûÂä†"],
                key_performance_indicators={"NPS": "45", "Êª°ÊÑèÂ∫¶": "75%"}
            )
        )

    def test_valid_complete_response(self):
        """Test creation of valid complete response."""
        response = self.create_complete_response()

        assert response.response_id == "test_response_123"
        assert response.nps_metrics.nps_score == 45
        assert len(response.foundation_insights) == 1
        assert len(response.analysis_insights) == 1
        assert len(response.consulting_recommendations) == 1

    def test_response_serialization(self):
        """Test response serialization to dict and JSON."""
        response = self.create_complete_response()

        # Test dict conversion
        response_dict = response.dict()
        assert response_dict["response_id"] == "test_response_123"
        assert response_dict["nps_metrics"]["nps_score"] == 45
        assert isinstance(response_dict["foundation_insights"], list)

        # Test JSON serialization
        json_str = response.json()
        parsed_data = json.loads(json_str)
        assert parsed_data["response_id"] == "test_response_123"
        assert parsed_data["nps_metrics"]["nps_score"] == 45

    def test_response_deserialization(self):
        """Test response deserialization from dict and JSON."""
        original_response = self.create_complete_response()
        response_dict = original_response.dict()

        # Test dict deserialization
        deserialized_response = NPSAnalysisResponse(**response_dict)
        assert deserialized_response.response_id == original_response.response_id
        assert deserialized_response.nps_metrics.nps_score == original_response.nps_metrics.nps_score

        # Test JSON deserialization
        json_str = original_response.json()
        parsed_dict = json.loads(json_str)
        json_deserialized_response = NPSAnalysisResponse(**parsed_dict)
        assert json_deserialized_response.response_id == original_response.response_id

    def test_minimal_response_creation(self):
        """Test creation of minimal response with required fields only."""
        minimal_response = NPSAnalysisResponse(
            response_id="minimal_123",
            nps_metrics=NPSMetrics(
                nps_score=0,
                promoter_count=0,
                passive_count=0,
                detractor_count=0,
                sample_size=0
            ),
            executive_dashboard=ExecutiveDashboard(
                executive_summary="ÊúÄÂ∞èÊëòË¶Å",
                top_recommendations=[],
                risk_alerts=[],
                key_performance_indicators={}
            )
        )

        assert minimal_response.response_id == "minimal_123"
        assert minimal_response.confidence_assessment is None  # Optional field
        assert len(minimal_response.foundation_insights) == 0  # Default empty list


class TestUtilityFunctions:
    """Test utility functions for model creation and validation."""

    def test_build_analysis_response(self):
        """Test build_analysis_response utility function."""
        # Test data
        response_data = {
            "response_id": "built_response",
            "nps_score": 60,
            "promoter_count": 70,
            "passive_count": 20,
            "detractor_count": 10,
            "sample_size": 100,
            "executive_summary": "ËâØÂ•ΩÁöÑÂÆ¢Êà∑Êª°ÊÑèÂ∫¶Ë°®Áé∞",
            "insights": [
                {
                    "agent_id": "B1",
                    "title": "ÂÆ¢Êà∑Êª°ÊÑèÂ∫¶ÂàÜÊûê",
                    "summary": "ÂÆ¢Êà∑Êï¥‰ΩìÊª°ÊÑèÂ∫¶ËæÉÈ´ò",
                    "content": "ËØ¶ÁªÜÁöÑÂÆ¢Êà∑Êª°ÊÑèÂ∫¶ÂàÜÊûêÁªìÊûúÂíåÂª∫ËÆÆÊé™ÊñΩ„ÄÇ",
                    "category": "ÂÆ¢Êà∑ÂàÜÊûê",
                    "priority": "‰∏≠",
                    "confidence": 0.8,
                    "impact_score": 0.7
                }
            ],
            "recommendations": [
                {
                    "title": "Áª¥Êä§ÂÆ¢Êà∑ÂÖ≥Á≥ª",
                    "description": "ÁªßÁª≠Áª¥Êä§Áé∞ÊúâÁöÑËâØÂ•ΩÂÆ¢Êà∑ÂÖ≥Á≥ª",
                    "priority": "‰∏≠",
                    "confidence_score": 0.75
                }
            ]
        }

        response = build_analysis_response(response_data)

        assert response.response_id == "built_response"
        assert response.nps_metrics.nps_score == 60
        assert len(response.analysis_insights) == 1
        assert len(response.consulting_recommendations) == 1

    def test_create_minimal_response(self):
        """Test create_minimal_response utility function."""
        minimal = create_minimal_response(
            response_id="minimal_test",
            nps_score=25,
            summary="ÊúÄÂ∞èÂìçÂ∫îÊµãËØï"
        )

        assert minimal.response_id == "minimal_test"
        assert minimal.nps_metrics.nps_score == 25
        assert "ÊúÄÂ∞èÂìçÂ∫îÊµãËØï" in minimal.executive_dashboard.executive_summary

    def test_validate_agent_insights(self):
        """Test validate_agent_insights utility function."""
        # Valid insights
        valid_insights = [
            AgentInsight(
                agent_id="B1",
                title="Ê¥ûÂØü1",
                summary="ÊëòË¶Å1",
                content="ËØ¶ÁªÜÂÜÖÂÆπ1ÔºåÂåÖÂê´Ë∂≥Â§üÁöÑÂàÜÊûê‰ø°ÊÅØ„ÄÇ",
                category="ÂàÜÊûê",
                priority="È´ò",
                confidence=0.8,
                impact_score=0.7
            ),
            AgentInsight(
                agent_id="B2",
                title="Ê¥ûÂØü2",
                summary="ÊëòË¶Å2",
                content="ËØ¶ÁªÜÂÜÖÂÆπ2ÔºåÂåÖÂê´Ë∂≥Â§üÁöÑÂàÜÊûê‰ø°ÊÅØ„ÄÇ",
                category="ÂàÜÊûê",
                priority="‰∏≠",
                confidence=0.7,
                impact_score=0.6
            )
        ]

        validation_result = validate_agent_insights(valid_insights)
        assert validation_result["is_valid"]
        assert validation_result["total_insights"] == 2
        assert len(validation_result["issues"]) == 0

        # Invalid insights (duplicate agent IDs)
        invalid_insights = [
            AgentInsight(
                agent_id="B1",  # Duplicate
                title="Ê¥ûÂØü1",
                summary="ÊëòË¶Å1",
                content="ËØ¶ÁªÜÂÜÖÂÆπ1ÔºåÂåÖÂê´Ë∂≥Â§üÁöÑÂàÜÊûê‰ø°ÊÅØ„ÄÇ",
                category="ÂàÜÊûê",
                priority="È´ò",
                confidence=0.8,
                impact_score=0.7
            ),
            AgentInsight(
                agent_id="B1",  # Duplicate
                title="Ê¥ûÂØü2",
                summary="ÊëòË¶Å2",
                content="ËØ¶ÁªÜÂÜÖÂÆπ2ÔºåÂåÖÂê´Ë∂≥Â§üÁöÑÂàÜÊûê‰ø°ÊÅØ„ÄÇ",
                category="ÂàÜÊûê",
                priority="‰∏≠",
                confidence=0.7,
                impact_score=0.6
            )
        ]

        validation_result = validate_agent_insights(invalid_insights)
        assert not validation_result["is_valid"]
        assert len(validation_result["issues"]) > 0


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_unicode_content(self):
        """Test handling of Unicode content."""
        insight = AgentInsight(
            agent_id="B1",
            title="UnicodeÊµãËØïüöÄ",
            summary="ÂåÖÂê´emojiÂíåÁâπÊÆäÂ≠óÁ¨¶ÁöÑÊëòË¶Åüìä",
            content="ËØ¶ÁªÜÂÜÖÂÆπÂåÖÂê´ÂêÑÁßçÂ≠óÁ¨¶Ôºö‰∏≠Êñá„ÄÅEnglish„ÄÅÊï∞Â≠ó123„ÄÅÁ¨¶Âè∑@#$%„ÄÅemojiüéØüìàüí°",
            category="ÁâπÊÆäÊµãËØï",
            priority="‰∏≠",
            confidence=0.5,
            impact_score=0.5
        )

        # Should handle Unicode properly
        assert "üöÄ" in insight.title
        assert "üìä" in insight.summary
        assert "üí°" in insight.content

    def test_very_long_content(self):
        """Test handling of very long content."""
        long_content = "ËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÈïøÁöÑÂÜÖÂÆπ„ÄÇ" * 1000  # Very long string

        insight = AgentInsight(
            agent_id="B1",
            title="ÈïøÂÜÖÂÆπÊµãËØï",
            summary="ÊµãËØïÈïøÂÜÖÂÆπÂ§ÑÁêÜ",
            content=long_content,
            category="ÊµãËØï",
            priority="‰Ωé",
            confidence=0.5,
            impact_score=0.5
        )

        assert len(insight.content) > 10000
        # Should not cause validation errors

    def test_boundary_values(self):
        """Test boundary values for numeric fields."""
        # Test minimum values
        min_metrics = NPSMetrics(
            nps_score=-100,  # Minimum NPS
            promoter_count=0,
            passive_count=0,
            detractor_count=0,
            sample_size=0
        )
        assert min_metrics.nps_score == -100

        # Test maximum values
        max_metrics = NPSMetrics(
            nps_score=100,  # Maximum NPS
            promoter_count=1000000,
            passive_count=1000000,
            detractor_count=1000000,
            sample_size=3000000
        )
        assert max_metrics.nps_score == 100

    def test_none_values(self):
        """Test handling of None values in optional fields."""
        # Response with minimal data and None values
        response = NPSAnalysisResponse(
            response_id="none_test",
            nps_metrics=NPSMetrics(
                nps_score=0,
                promoter_count=0,
                passive_count=0,
                detractor_count=0,
                sample_size=0
            ),
            confidence_assessment=None,  # Optional
            foundation_insights=[],
            analysis_insights=[],
            consulting_recommendations=[],
            executive_dashboard=ExecutiveDashboard(
                executive_summary="Âü∫Á°ÄÊëòË¶Å",
                top_recommendations=[],
                risk_alerts=[],
                key_performance_indicators={}
            )
        )

        assert response.confidence_assessment is None
        assert len(response.foundation_insights) == 0


if __name__ == "__main__":
    # Run tests
    pytest.main([__file__, "-v", "-s", "--tb=short"])